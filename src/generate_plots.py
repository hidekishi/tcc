#!/usr/bin/env python3
"""
Script para gera√ß√£o de gr√°ficos de an√°lise de desempenho dos benchmarks OpenMP
Gera gr√°ficos de speedup por aplica√ß√£o e compara√ß√µes entre aplica√ß√µes
"""

import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path
import argparse
import sys

class BenchmarkPlotter:
    """Gerador de gr√°ficos para an√°lise de benchmarks OpenMP"""
    
    def __init__(self, results_file, output_dir="plots"):
        """
        Inicializa o plotador com arquivo de resultados
        
        Args:
            results_file: Caminho para o arquivo JSON com resultados
            output_dir: Diret√≥rio de sa√≠da para os gr√°ficos
        """
        self.results_file = Path(results_file)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Carregar resultados
        print(f"üìÅ Carregando resultados de: {self.results_file}")
        with open(self.results_file, 'r') as f:
            self.results = json.load(f)
        
        # Converter para DataFrame
        self.df = self._prepare_dataframe()
        
        # Configurar estilo dos gr√°ficos
        plt.style.use('default')
        sns.set_palette("husl")
        
    def _prepare_dataframe(self):
        """Prepara DataFrame a partir dos resultados JSON"""
        data = []
        for result in self.results:
            if result.get('success', False):
                data.append({
                    'benchmark': result['benchmark'],
                    'threads': result['threads'],
                    'problem_size': result['problem_size'],
                    'iteration': result['iteration'],
                    'wall_time': result['wall_time']
                })
        
        if not data:
            print("‚ùå Nenhum resultado v√°lido encontrado!")
            sys.exit(1)
        
        df = pd.DataFrame(data)
        
        # Calcular tempo m√©dio por configura√ß√£o
        df_avg = df.groupby(['benchmark', 'threads', 'problem_size'])['wall_time'].mean().reset_index()
        df_avg.columns = ['benchmark', 'threads', 'problem_size', 'avg_time']
        
        # Calcular speedup (tempo com 1 thread / tempo com N threads)
        speedups = []
        for benchmark in df_avg['benchmark'].unique():
            for size in df_avg['problem_size'].unique():
                # Filtrar dados deste benchmark e tamanho
                subset = df_avg[(df_avg['benchmark'] == benchmark) & 
                               (df_avg['problem_size'] == size)]
                
                if len(subset) == 0:
                    continue
                
                # Tempo base (1 thread)
                base_time = subset[subset['threads'] == 1]['avg_time'].values
                
                if len(base_time) == 0:
                    continue
                
                base_time = base_time[0]
                
                # Calcular speedup para cada configura√ß√£o de threads
                for _, row in subset.iterrows():
                    speedup = base_time / row['avg_time']
                    efficiency = (speedup / row['threads']) * 100
                    
                    speedups.append({
                        'benchmark': row['benchmark'],
                        'threads': row['threads'],
                        'problem_size': row['problem_size'],
                        'avg_time': row['avg_time'],
                        'speedup': speedup,
                        'efficiency': efficiency
                    })
        
        return pd.DataFrame(speedups)
    
    def plot_speedup_per_application(self):
        """
        Gera gr√°ficos individuais de speedup para cada aplica√ß√£o
        Cada gr√°fico mostra todas as dimens√µes (small, medium, large, etc.)
        """
        print("\nüìä Gerando gr√°ficos individuais por aplica√ß√£o...")
        
        benchmarks = self.df['benchmark'].unique()
        problem_sizes = sorted(self.df['problem_size'].unique())
        
        # Cores e estilos para cada tamanho
        size_colors = {
            'small': '#2ecc71',
            'medium': '#3498db', 
            'large': '#e74c3c',
            'huge': '#9b59b6',
            'extreme': '#e67e22'
        }
        
        size_markers = {
            'small': 'o',
            'medium': 's',
            'large': '^',
            'huge': 'D',
            'extreme': 'v'
        }
        
        for benchmark in benchmarks:
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            fig.suptitle(f'{benchmark} - An√°lise de Escalabilidade', fontsize=14, fontweight='bold')
            
            # Filtrar dados deste benchmark
            bench_data = self.df[self.df['benchmark'] == benchmark]
            
            # Plot 1: Speedup vs Threads
            for size in problem_sizes:
                size_data = bench_data[bench_data['problem_size'] == size].sort_values('threads')
                
                if len(size_data) == 0:
                    continue
                
                ax1.plot(size_data['threads'], size_data['speedup'], 
                        marker=size_markers.get(size, 'o'),
                        color=size_colors.get(size, 'gray'),
                        linewidth=2, markersize=8,
                        label=f'{size}')
            
            # Linha de speedup ideal
            max_threads = bench_data['threads'].max()
            ideal_threads = sorted(bench_data['threads'].unique())
            ax1.plot(ideal_threads, ideal_threads, 'k--', 
                    linewidth=1.5, alpha=0.7, label='Speedup Ideal')
            
            ax1.set_xlabel('N√∫mero de Threads', fontsize=11)
            ax1.set_ylabel('Speedup', fontsize=11)
            ax1.set_title('Speedup vs Threads', fontsize=12)
            ax1.legend(loc='upper left', fontsize=9)
            ax1.grid(True, alpha=0.3)
            ax1.set_xscale('log', base=2)
            ax1.set_yscale('log', base=2)
            
            # Plot 2: Efici√™ncia Paralela
            for size in problem_sizes:
                size_data = bench_data[bench_data['problem_size'] == size].sort_values('threads')
                
                if len(size_data) == 0:
                    continue
                
                ax2.plot(size_data['threads'], size_data['efficiency'],
                        marker=size_markers.get(size, 'o'),
                        color=size_colors.get(size, 'gray'),
                        linewidth=2, markersize=8,
                        label=f'{size}')
            
            # Linha de 100% efici√™ncia
            ax2.axhline(y=100, color='k', linestyle='--', linewidth=1.5, alpha=0.7, label='100% Efici√™ncia')
            
            ax2.set_xlabel('N√∫mero de Threads', fontsize=11)
            ax2.set_ylabel('Efici√™ncia Paralela (%)', fontsize=11)
            ax2.set_title('Efici√™ncia vs Threads', fontsize=12)
            ax2.legend(loc='upper right', fontsize=9)
            ax2.grid(True, alpha=0.3)
            ax2.set_xscale('log', base=2)
            ax2.set_ylim(0, 110)
            
            plt.tight_layout()
            
            # Salvar gr√°fico
            output_file = self.output_dir / f'{benchmark}_speedup.png'
            plt.savefig(output_file, dpi=300, bbox_inches='tight')
            print(f"  ‚úì {benchmark}: {output_file}")
            plt.close()
    
    def plot_standard_applications_comparison(self):
        """
        Gera gr√°ficos comparando todas as vers√µes 'standard' das aplica√ß√µes
        Um gr√°fico por tamanho de problema (small, medium, large, etc.)
        """
        print("\nüìä Gerando gr√°ficos comparativos das vers√µes standard...")
        
        # Filtrar apenas vers√µes standard (sem _fine ou _coarse no nome)
        standard_benchmarks = [b for b in self.df['benchmark'].unique() 
                              if not ('_fine' in b or '_coarse' in b)]
        
        problem_sizes = sorted(self.df['problem_size'].unique())
        
        # Cores diferentes para cada aplica√ß√£o
        colors = sns.color_palette("husl", len(standard_benchmarks))
        color_map = dict(zip(standard_benchmarks, colors))
        
        for size in problem_sizes:
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
            fig.suptitle(f'Compara√ß√£o de Aplica√ß√µes Standard - Problema Size: {size.upper()}', 
                        fontsize=14, fontweight='bold')
            
            # Plot 1: Speedup
            for i, benchmark in enumerate(standard_benchmarks):
                bench_data = self.df[(self.df['benchmark'] == benchmark) & 
                                    (self.df['problem_size'] == size)].sort_values('threads')
                
                if len(bench_data) == 0:
                    continue
                
                ax1.plot(bench_data['threads'], bench_data['speedup'],
                        marker='o', color=color_map[benchmark],
                        linewidth=2, markersize=7,
                        label=benchmark)
            
            # Speedup ideal
            max_threads = self.df['threads'].max()
            ideal_threads = sorted(self.df['threads'].unique())
            ax1.plot(ideal_threads, ideal_threads, 'k--',
                    linewidth=1.5, alpha=0.7, label='Speedup Ideal')
            
            ax1.set_xlabel('N√∫mero de Threads', fontsize=11)
            ax1.set_ylabel('Speedup', fontsize=11)
            ax1.set_title('Speedup vs Threads', fontsize=12)
            ax1.legend(loc='upper left', fontsize=8, ncol=2)
            ax1.grid(True, alpha=0.3)
            ax1.set_xscale('log', base=2)
            ax1.set_yscale('log', base=2)
            
            # Plot 2: Efici√™ncia
            for i, benchmark in enumerate(standard_benchmarks):
                bench_data = self.df[(self.df['benchmark'] == benchmark) & 
                                    (self.df['problem_size'] == size)].sort_values('threads')
                
                if len(bench_data) == 0:
                    continue
                
                ax2.plot(bench_data['threads'], bench_data['efficiency'],
                        marker='o', color=color_map[benchmark],
                        linewidth=2, markersize=7,
                        label=benchmark)
            
            ax2.axhline(y=100, color='k', linestyle='--', linewidth=1.5, alpha=0.7, label='100% Efici√™ncia')
            
            ax2.set_xlabel('N√∫mero de Threads', fontsize=11)
            ax2.set_ylabel('Efici√™ncia Paralela (%)', fontsize=11)
            ax2.set_title('Efici√™ncia vs Threads', fontsize=12)
            ax2.legend(loc='upper right', fontsize=8, ncol=2)
            ax2.grid(True, alpha=0.3)
            ax2.set_xscale('log', base=2)
            ax2.set_ylim(0, 110)
            
            plt.tight_layout()
            
            # Salvar gr√°fico
            output_file = self.output_dir / f'comparison_standard_{size}.png'
            plt.savefig(output_file, dpi=300, bbox_inches='tight')
            print(f"  ‚úì {size}: {output_file}")
            plt.close()
    
    def plot_granularity_comparison(self):
        """
        Gera gr√°ficos comparando as tr√™s vers√µes de cada aplica√ß√£o
        (standard, fine-grained, coarse-grained)
        """
        print("\nüìä Gerando gr√°ficos de compara√ß√£o de granularidade...")
        
        # Identificar aplica√ß√µes base (sem sufixo)
        base_apps = set()
        for bench in self.df['benchmark'].unique():
            if '_fine' in bench:
                base_apps.add(bench.replace('_fine', ''))
            elif '_coarse' in bench:
                base_apps.add(bench.replace('_coarse', ''))
            else:
                # Verificar se existem variantes fine/coarse
                bench_base = bench
                if f"{bench}_fine" in self.df['benchmark'].values or f"{bench}_coarse" in self.df['benchmark'].values:
                    base_apps.add(bench_base)
        
        problem_sizes = sorted(self.df['problem_size'].unique())
        
        for base_app in sorted(base_apps):
            # Identificar as tr√™s vers√µes
            variants = {
                'standard': base_app,
                'fine': f"{base_app}_fine",
                'coarse': f"{base_app}_coarse"
            }
            
            # Verificar se todas as vers√µes existem
            available_variants = {k: v for k, v in variants.items() 
                                 if v in self.df['benchmark'].values}
            
            if len(available_variants) < 2:
                continue
            
            # Criar subplots por tamanho de problema
            n_sizes = len(problem_sizes)
            fig, axes = plt.subplots(1, n_sizes, figsize=(6*n_sizes, 5))
            if n_sizes == 1:
                axes = [axes]
            
            fig.suptitle(f'{base_app} - Compara√ß√£o de Granularidade', 
                        fontsize=14, fontweight='bold')
            
            variant_colors = {
                'standard': '#3498db',
                'fine': '#2ecc71',
                'coarse': '#e74c3c'
            }
            
            variant_markers = {
                'standard': 'o',
                'fine': '^',
                'coarse': 's'
            }
            
            for idx, size in enumerate(problem_sizes):
                ax = axes[idx]
                
                for variant_name, variant_bench in available_variants.items():
                    variant_data = self.df[(self.df['benchmark'] == variant_bench) & 
                                          (self.df['problem_size'] == size)].sort_values('threads')
                    
                    if len(variant_data) == 0:
                        continue
                    
                    ax.plot(variant_data['threads'], variant_data['speedup'],
                           marker=variant_markers[variant_name],
                           color=variant_colors[variant_name],
                           linewidth=2, markersize=8,
                           label=variant_name)
                
                # Speedup ideal
                ideal_threads = sorted(self.df['threads'].unique())
                ax.plot(ideal_threads, ideal_threads, 'k--',
                       linewidth=1.5, alpha=0.7, label='ideal')
                
                ax.set_xlabel('Threads', fontsize=10)
                ax.set_ylabel('Speedup', fontsize=10)
                ax.set_title(f'{size}', fontsize=11)
                ax.legend(loc='upper left', fontsize=8)
                ax.grid(True, alpha=0.3)
                ax.set_xscale('log', base=2)
                ax.set_yscale('log', base=2)
            
            plt.tight_layout()
            
            # Salvar gr√°fico
            output_file = self.output_dir / f'{base_app}_granularity_comparison.png'
            plt.savefig(output_file, dpi=300, bbox_inches='tight')
            print(f"  ‚úì {base_app}: {output_file}")
            plt.close()
    
    def plot_execution_time_heatmap(self):
        """
        Gera heatmaps de tempo de execu√ß√£o para cada tamanho de problema
        """
        print("\nüìä Gerando heatmaps de tempo de execu√ß√£o...")
        
        # Filtrar apenas vers√µes standard
        standard_benchmarks = [b for b in self.df['benchmark'].unique() 
                              if not ('_fine' in b or '_coarse' in b)]
        
        problem_sizes = sorted(self.df['problem_size'].unique())
        
        for size in problem_sizes:
            # Preparar dados para o heatmap
            pivot_data = self.df[(self.df['benchmark'].isin(standard_benchmarks)) & 
                                (self.df['problem_size'] == size)].pivot_table(
                                    index='benchmark',
                                    columns='threads',
                                    values='avg_time',
                                    aggfunc='mean'
                                )
            
            if pivot_data.empty:
                continue
            
            fig, ax = plt.subplots(figsize=(12, len(standard_benchmarks) * 0.6))
            
            sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='YlOrRd',
                       cbar_kws={'label': 'Tempo (segundos)'},
                       linewidths=0.5, ax=ax)
            
            ax.set_title(f'Tempo de Execu√ß√£o - {size.upper()}', fontsize=13, fontweight='bold')
            ax.set_xlabel('N√∫mero de Threads', fontsize=11)
            ax.set_ylabel('Aplica√ß√£o', fontsize=11)
            
            plt.tight_layout()
            
            # Salvar gr√°fico
            output_file = self.output_dir / f'heatmap_time_{size}.png'
            plt.savefig(output_file, dpi=300, bbox_inches='tight')
            print(f"  ‚úì {size}: {output_file}")
            plt.close()
    
    def generate_all_plots(self):
        """Gera todos os gr√°ficos dispon√≠veis"""
        print("\n" + "="*60)
        print("üé® GERADOR DE GR√ÅFICOS - BENCHMARK OPENMP")
        print("="*60)
        print(f"\nüìä Total de resultados carregados: {len(self.df)}")
        print(f"üìã Aplica√ß√µes encontradas: {len(self.df['benchmark'].unique())}")
        print(f"üî¢ Contagens de threads: {sorted(self.df['threads'].unique())}")
        print(f"üìè Tamanhos de problema: {sorted(self.df['problem_size'].unique())}")
        
        self.plot_speedup_per_application()
        self.plot_standard_applications_comparison()
        self.plot_granularity_comparison()
        self.plot_execution_time_heatmap()
        
        print("\n" + "="*60)
        print(f"‚úÖ Todos os gr√°ficos foram salvos em: {self.output_dir.absolute()}")
        print("="*60)

def main():
    parser = argparse.ArgumentParser(
        description='Gerador de gr√°ficos para an√°lise de benchmarks OpenMP',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:
  # Gerar todos os gr√°ficos
  python generate_plots.py benchmark_results/benchmark_results_20241125_120000.json
  
  # Especificar diret√≥rio de sa√≠da
  python generate_plots.py results.json --output plots_final
  
  # Gerar apenas gr√°ficos espec√≠ficos
  python generate_plots.py results.json --speedup-only
  python generate_plots.py results.json --comparison-only
        """
    )
    
    parser.add_argument('results_file', 
                       help='Arquivo JSON com resultados dos benchmarks')
    parser.add_argument('--output', '-o', default='plots',
                       help='Diret√≥rio de sa√≠da para os gr√°ficos (default: plots)')
    parser.add_argument('--speedup-only', action='store_true',
                       help='Gerar apenas gr√°ficos individuais de speedup')
    parser.add_argument('--comparison-only', action='store_true',
                       help='Gerar apenas gr√°ficos comparativos')
    parser.add_argument('--granularity-only', action='store_true',
                       help='Gerar apenas gr√°ficos de compara√ß√£o de granularidade')
    
    args = parser.parse_args()
    
    # Verificar se arquivo existe
    results_path = Path(args.results_file)
    if not results_path.exists():
        print(f"‚ùå Erro: Arquivo n√£o encontrado: {results_path}")
        sys.exit(1)
    
    # Criar plotador
    plotter = BenchmarkPlotter(args.results_file, args.output)
    
    # Gerar gr√°ficos conforme op√ß√µes
    if args.speedup_only:
        plotter.plot_speedup_per_application()
    elif args.comparison_only:
        plotter.plot_standard_applications_comparison()
    elif args.granularity_only:
        plotter.plot_granularity_comparison()
    else:
        plotter.generate_all_plots()

if __name__ == "__main__":
    main()
