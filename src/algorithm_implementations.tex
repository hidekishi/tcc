\section{Implementação dos Algoritmos (Versão Padrão)}

Esta seção detalha a implementação OpenMP de cada algoritmo na sua \textbf{versão padrão (standard)}, explicando as estratégias de paralelização utilizadas.

\subsection{Cálculo de Pi - Integração Numérica}

\textbf{Método:} Integração numérica da função $f(x) = \frac{4}{1+x^2}$ no intervalo $[0,1]$ usando a regra do retângulo.

\textbf{Paralelização OpenMP:}
\begin{lstlisting}[language=C]
#pragma omp parallel for default(shared) private(i, local) reduction(+:pi)
for(i = 0; i < N; i++) {
    local = (i + 0.5) * w;
    pi += 4.0 / (1.0 + local * local);
}
\end{lstlisting}

A diretiva \texttt{parallel for} distribui as iterações entre threads, com cada thread acumulando sua contribuição em uma cópia local através da cláusula \texttt{reduction(+:pi)}, que realiza a soma final automaticamente. As variáveis \texttt{i} e \texttt{local} são declaradas como \texttt{private} para garantir que cada thread trabalhe com suas próprias instâncias. O scheduling implícito \texttt{static} divide o trabalho em chunks contíguos, adequado para este workload perfeitamente balanceado onde cada iteração tem custo computacional uniforme.

O padrão de acesso à memória é altamente favorável: não há dependências de dados entre iterações e o acesso é sequencial (cache-friendly). A única contenção ocorre na sincronização final da reduction.

%-------------------------------------------------------------------

\subsection{Mandelbrot Set - Monte Carlo Sampling}

\textbf{Método:} Amostragem Monte Carlo para estimar área do conjunto de Mandelbrot. Cada ponto é testado pela iteração $z_{n+1} = z_n^2 + c$ até escapar ($|z| > 2$) ou atingir MAXITER iterações.

\textbf{Paralelização OpenMP:}
\begin{lstlisting}[language=C]
#pragma omp parallel for default(none) reduction(+:outside) \
                         private(i, j, ztemp, z) shared(NPOINTS, points)
for(i = 0; i < NPOINTS; i++) {
    for (j = 0; j < MAXITER; j++) {
        // Iteracao z = z^2 + c
        ztemp = (z.re * z.re) - (z.im * z.im) + points[i].re;
        z.im = z.re * z.im * 2 + points[i].im;
        z.re = ztemp;
        if (z.re * z.re + z.im * z.im > THRESHOLD) {
            outside++; break;
        }
    }
}
\end{lstlisting}

Esta implementação usa \texttt{default(none)} para forçar a especificação explícita de todas as variáveis, evitando compartilhamentos acidentais. O problema é embaraçosamente paralelo, com pontos independentes entre si, mas apresenta workload irregular pois cada ponto diverge em um número diferente de iterações (entre 1 e MAXITER). O loop interno não pode ser paralelizado devido à dependência temporal onde $z_{n+1}$ depende de $z_n$.

O array \texttt{points[]} é acessado em modo read-only (compartilhado), enquanto as variáveis \texttt{z} e \texttt{ztemp} são privadas a cada thread. Embora o workload desbalanceado normalmente favorecesse dynamic scheduling, a versão standard usa static: com muitos pontos (2M+), o desbalanceamento se ameniza estatisticamente, e o overhead de dynamic scheduling não compensa, além de static proporcionar melhor cache locality.

%-------------------------------------------------------------------

\subsection{QuickSort - Ordenação Paralela}

\textbf{Método:} Algoritmo divide-and-conquer recursivo com paralelização baseada em tasks. Usa cutoff threshold para evitar overhead em partições pequenas.

\textbf{Paralelização OpenMP:}
\begin{lstlisting}[language=C]
void quicksort_tasks(int *v, int left, int right, int cutoff) {
    if (left < right) {
        int pivot_index = partition(v, left, right);
        #pragma omp task shared(v) if(right - left > cutoff)
        quicksort_tasks(v, left, pivot_index - 1, cutoff);
        #pragma omp task shared(v) if(right - left > cutoff)
        quicksort_tasks(v, pivot_index + 1, right, cutoff);
        #pragma omp taskwait
    }
}
\end{lstlisting}

O algoritmo implementa task-based parallelism através de \texttt{\#pragma omp task}, permitindo recursão paralela nas duas partições geradas. A cláusula \texttt{if(right - left > cutoff)} aplica um threshold (aproximadamente 10.000 elementos na versão standard) para evitar criar tasks quando as partições são pequenas, situação onde o overhead de criação supera os ganhos de paralelização. A diretiva \texttt{taskwait} sincroniza a thread pai com as subtasks, garantindo que ambas as partições estejam ordenadas antes de retornar. O load balancing é gerenciado automaticamente pela work-stealing queue do runtime OpenMP.

O padrão de acesso à memória apresenta desafios: durante o partition ocorrem acessos não sequenciais causando cache misses, e como a ordenação é in-place existe potencial para false sharing. No entanto, após o partition, as partições tornam-se independentes permitindo paralelismo sem conflitos.

%-------------------------------------------------------------------

\subsection{FFT - Fast Fourier Transform}

\textbf{Método:} Algoritmo Cooley-Tukey recursivo (Radix-2) com complexidade $O(N \log N)$. Divide em pares/ímpares, aplica FFT recursiva e combina com twiddle factors.

\textbf{Paralelização OpenMP (3 níveis):}
\begin{lstlisting}[language=C]
// 1. Split: separar pares e impares
#pragma omp parallel for
for (int i = 0; i < N/2; i++) {
    even[i] = x[2*i]; odd[i] = x[2*i + 1];
}

// 2. Conquer: FFT recursiva paralela
#pragma omp task shared(even)
fft_parallel(even, N/2, cutoff);
#pragma omp task shared(odd)
fft_parallel(odd, N/2, cutoff);
#pragma omp taskwait

// 3. Combine: aplicar twiddle factors
#pragma omp parallel for
for (int k = 0; k < N/2; k++) {
    complex t = twiddle[k] * odd[k];
    x[k] = even[k] + t; x[k + N/2] = even[k] - t;
}
\end{lstlisting}

A implementação explora paralelização em três níveis distintos. No split, \texttt{parallel for} distribui a separação de elementos pares e ímpares entre threads. Na fase conquer, tasks paralelas executam as chamadas recursivas de FFT sobre as subsequências even e odd, com \texttt{taskwait} sincronizando antes do combine. Finalmente, o combine aplica os twiddle factors em paralelo via \texttt{parallel for}. A versão standard utiliza cutoff adaptativo de 4096 pontos, abaixo do qual a execução é serial. A complexidade $O(N \log N)$ torna a paralelização eficiente.

O padrão de acesso à memória é desafiador: o split realiza stride-2 access (cache não sequencial), e o combine segue o butterfly pattern característico da FFT, causando cache misses. Entretanto, as recursões são completamente independentes, permitindo paralelismo sem dependências de dados.

%-------------------------------------------------------------------

\subsection{Jacobi Iterative Solver}

\textbf{Método:} Solver iterativo para equações diferenciais parciais usando stencil 5-pontos. Cada ponto é atualizado pela média dos 4 vizinhos até convergência.

\textbf{Paralelização OpenMP:}
\begin{lstlisting}[language=C]
for (iter = 0; iter < max_iter; iter++) {
    #pragma omp parallel for private(i, j) shared(u, u_old, m, n)
    for (i = 1; i < m-1; i++) {
        for (j = 1; j < n-1; j++) {
            u[i][j] = 0.25 * (u_old[i-1][j] + u_old[i+1][j] +
                              u_old[i][j-1] + u_old[i][j+1]);
        }
    }
    #pragma omp parallel for
    for (i = 0; i < m; i++)
        memcpy(u_old[i], u[i], n * sizeof(double));
}
\end{lstlisting}

O método implementa stencil computation onde cada ponto é atualizado pela média dos 4 vizinhos. Existe dependência temporal entre iterações (K depende de K-1), impedindo paralelização no loop externo, mas há independência espacial: todos os pontos na mesma iteração podem ser calculados simultaneamente. A barrier implícita ao final do \texttt{parallel for} garante que todos os pontos foram atualizados antes da cópia para \texttt{u\_old}. O workload é perfeitamente uniforme pois cada ponto realiza exatamente 4 operações.

O acesso à memória é majoritariamente sequencial por linhas (cache-friendly), mas o acesso vertical ($u[i\pm1][j]$) pode causar cache misses dependendo do tamanho da grade. O balanceamento perfeito favorece static scheduling (default). Otimizações possíveis incluem red-black coloring para paralelizar iterações temporalmente e blocking/tiling para melhor cache utilization.

%-------------------------------------------------------------------

\subsection{LU Decomposition}

\textbf{Método:} Decomposição de matriz $A = L \times U$ (Lower $\times$ Upper triangular) usando algoritmo de Doolittle. Complexidade $O(N^3)$ com paralelização interna em cada nível k.

\textbf{Paralelização OpenMP:}
\begin{lstlisting}[language=C]
for (k = 0; k < N; k++) {
    // Linha k de U (paralelizavel)
    #pragma omp parallel for private(j, s) shared(U, L, A, k, N)
    for (j = k; j < N; j++) {
        U[k][j] = A[k][j];
        for (s = 0; s < k; s++)
            U[k][j] -= L[k][s] * U[s][j];
    }
    
    // Coluna k de L (paralelizavel apos linha k)
    #pragma omp parallel for private(i, s) shared(U, L, A, k, N)
    for (i = k+1; i < N; i++) {
        L[i][k] = A[i][k];
        for (s = 0; s < k; s++)
            L[i][k] -= L[i][s] * U[s][k];
        L[i][k] /= U[k][k];
    }
}
\end{lstlisting}

O algoritmo apresenta dependência por nível onde cada iteração k deve completar antes de k+1, forçando serialização do loop externo. Entretanto, dentro de cada nível k, o cálculo das linhas de U e colunas de L pode ser feito em paralelo pois são independentes entre si. A barrier implícita entre as duas regiões paralelas garante que a linha k de U esteja completa antes de calcular a coluna k de L. O workload decresce a cada iteração k mas as operações são densas e compute-bound, com alta intensidade aritmética.

O acesso por linhas na matriz U é cache-friendly, mas o acesso por colunas em L é não sequencial, causando potenciais cache misses. Os loops internos apresentam boa reutilização de dados. O principal desafio é que apenas os loops internos são paralelizáveis, com o loop externo permanecendo serial, limitando o speedup pela Lei de Amdahl.

%-------------------------------------------------------------------

\subsection{Molecular Dynamics - N-body Simulation}

\textbf{Método:} Simulação N-body com forças de Lennard-Jones ($F = 24\epsilon[(2(\sigma/r)^{13} - (\sigma/r)^7)]$). Complexidade $O(N^2)$ para cálculo de forças entre todos os pares, seguido de integração Verlet.

\textbf{Paralelização OpenMP:}
\begin{lstlisting}[language=C]
for (step = 0; step < n_steps; step++) {
    // Calcular forcas entre pares
    #pragma omp parallel for private(i, j, r, dist, f_mag, f) \
                             schedule(dynamic, 64)
    for (i = 0; i < n_particles; i++) {
        force[i] = {0, 0, 0};
        for (j = i+1; j < n_particles; j++) {
            // Calculo Lennard-Jones e acumulo atomico
            #pragma omp atomic
            force[i].x += f.x;
            #pragma omp atomic
            force[j].x -= f.x;  // F_ij = -F_ji
        }
    }
    
    // Integracao Verlet
    #pragma omp parallel for
    for (i = 0; i < n_particles; i++) {
        vel[i] += force[i] / mass[i] * dt;
        pos[i] += vel[i] * dt;
    }
}
\end{lstlisting}

O cálculo de forças apresenta complexidade $O(N^2)$ ao computar interações entre todos os pares de partículas. Como múltiplas threads podem escrever em \texttt{force[j]} simultaneamente (pela Lei de Newton $F_{ij} = -F_{ji}$), existe race condition resolvida com operações \texttt{atomic}, embora com overhead considerável. Alternativas incluem usar arrays privados de força com reduction posterior, ou aplicar spatial partitioning (cell lists) reduzindo a complexidade para $O(N)$.

O padrão de acesso à memória é irregular: \texttt{pos[j]} é acessado aleatoriamente causando cache misses frequentes, e pode haver false sharing no array \texttt{force[]} caso não seja adequadamente padded. A fase de integração Verlet é perfeitamente paralela sem dependências. A versão standard usa dynamic scheduling com chunk size 64 para lidar com o workload desbalanceado, pois partículas podem ter diferentes números de vizinhos próximos, balanceando overhead contra distribuição de carga.

%-------------------------------------------------------------------

\subsection{Graph Search - BFS Level-Synchronous}

\textbf{Método:} Busca em largura (BFS) processando um nível do grafo por vez. Complexidade $O(V + E)$ onde V é o número de vértices e E o número de arestas.

\textbf{Paralelização OpenMP:}
\begin{lstlisting}[language=C]
while (curr_size > 0) {
    next_size = 0;
    
    // Processar nivel atual em paralelo
    #pragma omp parallel for reduction(+:next_size)
    for (int i = 0; i < curr_size; i++) {
        int u = current_level[i];
        for (int j = 0; j < g->adj[u].size; j++) {
            int v = g->adj[u].nodes[j];
            
            // Atomic para evitar race condition
            bool was_visited;
            #pragma omp atomic capture
            { was_visited = visited[v]; visited[v] = true; }
            
            if (!was_visited) {
                int pos;
                #pragma omp atomic capture
                pos = next_size++;
                next_level[pos] = v;
            }
        }
    }
    swap(current_level, next_level);
    curr_size = next_size;
}
\end{lstlisting}

A abordagem level-synchronous processa todos os vértices de um nível do grafo por vez, permitindo paralelização dentro de cada nível. O workload é altamente irregular pois vértices têm graus diferentes, dificultando o balanceamento de carga. Como múltiplas threads podem tentar visitar o mesmo vértice simultaneamente, \texttt{atomic capture} resolve a race condition tanto para o array \texttt{visited[]} quanto para o contador \texttt{next\_size}. A barrier implícita ao final do \texttt{parallel for} sincroniza antes de processar o próximo nível.

O padrão de acesso à memória é completamente irregular, dependendo da topologia do grafo, resultando em cache misses muito frequentes e tornando o balanceamento de carga particularmente desafiador. A paralelização eficiente de grafos permanece tema de pesquisa ativo: o speedup é limitado para grafos pequenos, sendo mais efetivo em grafos grandes e densos onde o volume de trabalho por nível justifica o overhead de sincronização.
