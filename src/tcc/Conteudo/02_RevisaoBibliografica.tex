\chapter{Revisão Bibliográfica}
\label{revisao_bibliografica}

Nesse capítulo são analisadas diversas fontes afim de ter uma melhor compreensão sobre a área de computação paralela, suas sub-áreas de estudo, aplicações, problemas e lacunas a serem estudadas nesse trabalho.
% ---
\section{Computação Paralela}
% ---

A computação paralela  tem se firmado como uma das principais áreas da computação, tendo como objetivo explorar a combinação de recursos computacionais para execução simultânea de tarefas mirando a resolução de problemas de grande porte. Diferente da execução sequencial, que limita a execução integral de um programa a apenas uma unidade de processamento por vez, a execução paralela divide tarefas em partes menores que são distribuídas e executadas simultaneamente por diversas unidades, o que permite reduzir significativamente o tempo de execução e permitir que problemas anteriormente inviáveis serem então solucionáveis em tempo hábil por máquinas \cite{hennessypatterson2017, kumargramagupta1994}.

Previamente à computação paralela, o ganho de desempenho computacional se baseava no aumento de frequência do \textit{clock} dos processadores, porém questões de eficiência energética e dissipação de calor tornaram a linearidade desse crescimento insustentável. Essas limitações eventualmente levaram ao desenvolvimento e adoção de arquiteturas multicore, GPUs e sistemas distribuídos afim de manter o ganho de performance vivo \cite{sutter2005, asanovic2009}. Inicialmente um recurso exclusivo para supercomputadores, atualmente o padrão da indústria são dispositivos multicore \cite{kirkhwu2010}.

Apesar de ser uma solução, a computação paralela também esbarra em algumas limitações. A Lei de \citeonline{amdahl1967} introduziu a ideia de que o ganho de desempenho máximo de um programa paralelo, o chamado \textit{speedup}, é limitado pela fração sequencial do código, enquanto a Lei de \citeonline{gustafson1988} trouxe um contraponto a Amdahl afirmando que esse ganho deveria levar em consideração o tamanho de entrada do problema.

Outro aspecto importante da área é referente a arquiteturas de hardware e modelos de programação. Dentro das arquiteturas, são dois tipos principais: Sistemas de memória compartilhada, como os multiprocessadores simétricos (SMP), de memória distribuída, como clusters conectados por rede \cite{tanenbaumsteen2006} e processamento em GPUs. As diferentes arquiteturas pedem por diferentes modelos de programação, onde se destacam o OpenMP para ambientes de memória compartilhada \cite{dagummenon1998}, o MPI para memória distribuída \cite{groppluskskjellum2014} e o CUDA para programação em GPUs \cite{nickolletal2008}. Cada paradigma impõe diferentes estratégias de paralelização e apresentam diferentes prós e contras.

Para analisar as vantagens e disvantagens além de melhores casos de uso para cada arquitetura e modelo, a avaliação de desempenho é uma peça chave. Métricas como \textit{speedup} e eficiência fornecem uma noção matemática mais direta para a avaliação \cite{kumargramagupta1994}. Porém, a aplicação prática e utilização dessas métricas de forma efetiva depende de uma padronização para as comparações entre diferentes arquiteturas e algoritmos. Nesse cenário que surgem diferentes plataformas de benchmark, também conhecidas como suítes. Iniciativas como os NAS Parallel Benchmarks \cite{baileyetal1991}, da NASA, e o LINKPACK, utilizado por exemplo no projeto TOP500 \cite{dongarrameuerstrohmaier1999} se consolidaram como referências nesse processo, fornecendo plataformas e padrões para execução e avaliação de hardware.

\section{OpenMP}

O \textit{OpenMP} (Open Multi-Processing) é um padrão de programação paralela desenvolvido para arquiteturas de memória compartilhada. Foi criado em 1997 por um consórcio de empresas e centros de pesquisa com o propósito de unificar diferentes propostas de diretivas de compilação já existentes para linguagens como C, C++ e Fortran. Sua característica principal é o uso de diretivas de compilador, o que permite paralelizar programas originalmente sequenciais de forma incremental facilmente sem precisar fazer grandes mudanças na lógica original \cite{dagummenon1998,terboven2017}.

 O OpenMP segue o paradigma \textit{fork-join}. A execução inicia em uma \textit{thread} mestre que, passando por uma diretiva paralela, cria múltiplas \textit{threads} trabalhadoras que executam a região paralela e se sincronizam no final das tarefas para retomar o fluxo sequencial. Esse mecanismo é adequado tanto para programas com laços independentes quanto para aplicações mais complexas, que exigem coordenação e comunicação entre tarefas \cite{hager2010}.

 As primeiras versões do OpenMP focavam apenas em laços e seções, mas com o passar do tempo foi recebendo atualizações introduzindo conceitos como \textit{tasks}, para paralelismo dinâmico como em algoritmos recursivos, recursos de heterogeneidade, dependências explícitas e, mais recentemente, gerenciamento de memória e interoperabilidade com outras APIs. Sendo assim, o OpenMP se tornou uma ferramenta versátil para arquiteturas multicore \cite{chandrasekaran2017, terboven2017}. 

No ambiente acadêmico, o OpenMP se tornou padrão no ensino de programação paralela pela simplicidade e facilidade de aprendizado. Por conta da ampla disponibilidade de compiladores, também é muito utilizado em pesquisa e na indústria \cite{pacheco2011introduction}.

% ---
\section{Granularidade}
\label{sec-granularidade}
A \textbf{granularidade}, também chamada de \textit{grain size}, é um dos conceitos centrais no estudo de algoritmos paralelos. Ela representa a quantidade de trabalho computacional atribuída a cada unidade de paralelismo, seja uma tarefa, um thread ou um processo. Uma granularidade fina (\textit{fine-grained}) representa uma divisão maior de tarefas, enquanto uma granularidade grossa (\textit{coarse-grained}) indica uma menor compartimentalização do trabalho \cite{kumargramagupta1994}.

A escolha adequada da granularidade é primordial para o equilíbrio entre o aproveitamento do paralelismo e o custo de coordenação entre tarefas. Quando se adota granularidade muito fina, aumentam-se as oportunidades de paralelismo, mas também os custos de escalonamento, sincronização e comunicação, o que configura o \textit{overhead} (Seção \ref{sec-overhead}). Já uma granularidade muito grossa reduz esse custo, mas pode levar a desbalanceamento de carga e ociosidade de processadores \cite{kumargramagupta1994, quinn2003, hennessypatterson2017}.

Os estudos clássicos de \citeonline{amdahl1967} e \citeonline{gustafson1988}, citados anteriormente, já apontavam que o ganho de desempenho depende não apenas da fração paralelizável do código, mas também da forma como o trabalho é dividido entre processadores. Posteriormente, frameworks de custo como o BSP \cite{valiant1990} e o LogP \cite{culleretal1993}, reforçaram a ideia de que a granularidade deve ser ajustada de forma a mitigar os custos de comunicação e sincronização. Estudos sobre \textit{work stealing} \cite{blumoleiserson1999} mostram que tarefas excessivamente pequenas podem sobrecarregar o sistema de escalonamento, enquanto granularidade mais adequada favorece tanto a eficiência quanto a localidade de cache. Em ambientes baseados em MPI, trabalhos como os de \citeonline{thakurrabenseifnergropp2005} apontam que a granularidade impacta diretamente na relação entre computação e comunicação, definindo os limites de escalabilidade forte e fraca (Seção \ref{sec-escalabilidade}).

Na prática, o impacto da granularidade é perceptível em diferentes arquiteturas. Em sistemas de memória compartilhada granularidade ajustada ao tamanho dos blocos de cache pode reduzir misses e problemas de \textit{false sharing}. Já em clusters, tarefas mais agregadas podem mitigar a latência de rede e reduzir o número de sincronizações. E em GPUs, a granularidade está relacionada ao número de operações atribuídas por \textit{thread} ou bloco, influenciando fatores como \textit{occupancy} e divergência de execução em \textit{warps} \cite{mcoolreindersrobison2012, williamswatermanpatterson2009}.

Vários trabalhos propuseram métodos para diminuir os problemas gerados por granularidades escolhidas de maneira inadequada. Uma das estratégias mais comuns é a de \textit{cutoff} adaptativo em algoritmos recursivos em que problemas muito pequenos passam a ser processados de forma serial, assim reduzindo o custo de \textit{overhead} com a criação de tarefas \cite{frigoleisersonrandall1998}. Técnicas como \textit{guided self-scheduling} de \citeonline{polychronopouloskuck1987} podem ser utilizadas em sistemas baseados em loops como uma forma de ajustar dinamicamente o tamanho dos blocos distribuídos para cada processador afim de balancear a carga. Outro método relevante é o uso de \textit{tiling} (blocagem) para permitir uma melhor utilização da memória, otimizando o código \cite{hennessypatterson2017}.

Em exemplos práticos linguagens baseadas em tarefas, como Cilk, verificou-se que \textit{cutoffs} adequados resultam em ganhos de eficiência sem comprometer o speedup máximo \cite{frigoleisersonrandall1998}. Em aplicações científicas baseadas em MPI, a escolha de granularidade maior reduziu a frequência de comunicação e melhorou a escalabilidade \cite{thakurrabenseifnergropp2005}. Já no contexto do modelo \textit{Roofline}, observou-se que ajustes de granularidade, via técnicas como \textit{tiling} ou \textit{kernel fusion}, aumentam a intensidade aritmética e reduzem o acesso excessivo e pouco aproveitado por processo \cite{williamswatermanpatterson2009}.

\section{Overhead}
\label{sec-overhead}
O \textit{overhead} em sistemas paralelos se refere ao tempo gasto com atividades indiretas, necessárias para coordenar a execução de um algoritmo, mas que não contribuem diretamente para a solução do problema. Esse custo adicional pode surgir da comunicação entre processadores, necessária para transferir dados, da sincronização de tarefas com interdependência de dados, que envolve o uso de barreiras ou mecanismos de bloqueio, e do gerenciamento de threads: criação, destruição e controle, especialmente em algoritmos com granularidade fina \cite{kumargramagupta1994}.

O \textit{overhead} impacta diretamente o desempenho de algoritmos paralelos, reduzindo o \textit{speedup} e a eficiência. Mesmo pequenas partes seriais ou altos custos de sincronização podem limitar os ganhos do paralelismo, conforme dito por \citeonline{amdahl1967}. Por outro lado, aumentar o tamanho do problema tende a reduzir o efeito relativo do overhead, permitindo um melhor aproveitamento dos processadores, conforme \citeonline{gustafson1988}.

Uma maneira de reduzir os custos de \textit{overhead} como já foi dito é aumentar a granularidade das tarefas, que reduz os custos com comunicação e sincronização entre processadores. Porém, como citado, é necessário balancear o \textit{overhead} e a granularidade para evitar também desbalanceamento de carga.

\section{Benchmarking}

A avaliação de algoritmos e arquiteturas paralelas precisa de métodos padronizados que garantam comparações justas de desempenho. Para isso, os benchmarks paralelos são ferramentas fundamentais, pois permitem testar e comparar sistemas em condições controladas. Eles podem ser divididos em três tipos: sintéticos, baseados em kernels e de aplicações completas \cite{bailey1995, hwang1993}.

Um dos benchmarks mais importantes é o LINPACK, usado como base do ranking TOP500, que lista os supercomputadores mais poderosos do mundo \cite{dongarrameuerstrohmaier1999}. Ele mede o desempenho em operações de álgebra linear densa, principalmente na resolução de sistemas lineares. Apesar da relevância, recebe críticas por não refletir a diversidade das aplicações reais \cite{dongarraetal2011}.

Nesse cenário surgiram os NAS Parallel Benchmarks (NPB), criados pela NASA \cite{baileyetal1991}. Eles representam kernels e versões simplificadas de aplicações típicas em simulações científicas, como FFT, decomposição LU e gradiente conjugado. Diferente do LINPACK, o NPB busca avaliar padrões de comunicação e computação comuns em HPC, tornando-se referência para medir algoritmos e arquiteturas de forma mais ampla.

Outro conjunto importante é o SPEC OMP, desenvolvido pelo {Standard Performance Evaluation Corporation} \cite{henning2006}. Ele foca em cargas de trabalho paralelizadas com OpenMP, geralmente derivadas de aplicações reais de engenharia, física e análise de dados. Sua vantagem é estar mais próximo de problemas aplicados, mas exige mais cuidado na implementação e na configuração do ambiente de execução.

Mais recentemente, ganharam destaque benchmarks voltados para arquiteturas heterogêneas e GPGPU. O Parboil \cite{stratton2012} e o Rodinia \cite{che2009} reúnem kernels que simulam cargas de trabalho em áreas como aprendizado de máquina, processamento de imagens e simulações físicas. Essas suítes têm sido muito usadas em pesquisas atuais, pois permitem avaliar o desempenho em arquiteturas híbridas, explorando paralelismo em CPU e GPU.

Além dos benchmarks tradicionais, existem também os microbenchmarks, que analisam aspectos isolados do desempenho, como latência de comunicação, largura de banda da memória e custo de sincronização entre threads \cite{hoefler2010}. Embora não representem aplicações completas, são úteis para identificar gargalos em sistemas paralelos e ajustar a granularidade.

É possível concluir que, apesar de servirem ao mesmo propósito, cada benchmark tem direcionamentos próprios e adequados ao uso requerido.
% ---

\section{Métricas de Avaliação}

Além de estratégias para balancear a granularidade e \textit{overhead} é igualmente essencial avaliar de forma quantitativa o impacto dessas escolhas. Para isso, se consolidaram um conjunto de métricas utilizadas para comparar o desempenho de soluções paralelas em relação às suas versões sequenciais, estimar a escalabilidade diante do aumento de recursos computacionais e compreender os limites práticos impostos por fatores como comunicação, sincronização e balanceamento de carga.

\subsection{Speedup}

O \textit{speedup} é uma métrica que mede o ganho de desempenho obtido com a execução paralela em relação à execução sequencial. É definido pela razão entre o tempo de execução sequencial ($T_1$) e o tempo de execução paralelo ($T_p$), como mostra a Equação~\ref{eq-speedup}:

\begin{equation}
\label{eq-speedup}
    S = \frac{T_1}{T_p}
\end{equation}

Em um cenário ideal, o \textit{speedup} cresce linearmente com o número de processadores utilizados. No entanto, devido a fatores já citado e apontados por \citeonline{amdahl1967}, o \textit{speedup} real geralmente é menor que o ideal.

\subsection{Eficiência}

A eficiência ($E$) expressa o aproveitamento dos recursos paralelos disponíveis. É calculada pela razão entre o \textit{speedup} obtido e o número de processadores ($p$), conforme a Equação~\ref{eq-eficiencia}:

\begin{equation}
\label{eq-eficiencia}
    E = \frac{S}{p}
\end{equation}

Esse indicador varia entre 0 e 1, representando quão próximo o desempenho paralelo está do ideal. Valores baixos indicam que parte dos recursos está sendo desperdiçada devido a overhead, desequilíbrio de carga ou partes não paralelizáveis do código \cite{kumargramagupta1994}.

\section{Escalabilidade}
\label{sec-escalabilidade}

A escalabilidade sintetiza a relação entre algoritmos, arquiteturas e granularidade.  
Ela expressa até que ponto um sistema paralelo consegue aproveitar recursos adicionais sem perder eficiência, seja no aumento do número de processadores ou no crescimento do problema.  

Ela pode ser divida em duas: forte e fraca. Dizer que um sistema é fortemente escalável significa que ele mantém um grau alto de eficiência conforme aumenta a alocação de recursos computacionais sem a necessidade de aumentar a dimensão do problema, enquanto fracamente escalável se refere à relação de aumento do problema para manter essa eficiência em um nível aceitável \cite{gustafson1988}.

As métricas de \textit{speedup} e eficiência, além da granularidade oferecem meios quantitativos para avaliar a escalabilidade, enquanto benchmarks como LINPACK, NAS e PARSEC permitem validar tais análises em cenários reais.

\section{Trabalhos Relacionados}
\label{sec-trabalhos-relacionados}

Pesquisas sobre o impacto da granularidade na execução de diferentes cargas de trabalho têm sido exploradas no passado recente \cite{mittalvetter2014, ricogallegoetal2019}. Além disso, fatores como eficiência energética e escalabilidade em larga escala têm adquirido papel cada vez mais importante, muito por conta da transição à era de \textit{exascale computing} \cite{dongarraetal2011, shalf2020}.