%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Lauro Cesar Araujo at 2015-04-27 19:43:45 -0300 


%% Saved with string encoding Unicode (UTF-8) 

% ===========================================================
% Estes são os exemplos utilizados na seção de referências bibliográficas, como exemplos.
% ===========================================================
% Em artigos em revistas

@book{pacheco2011introduction,
  title={An introduction to parallel programming},
  author={Pacheco, Peter S},
  year={2011},
  publisher={Morgan Kaufmann}
}

@book{cormen2009introduction,
  title     = {Introduction to Algorithms},
  author    = {Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L. and Stein, Clifford},
  edition   = {3},
  year      = {2009},
  publisher = {MIT Press},
  address   = {Cambridge, MA}
}

@misc{merriam2025benchmark,
  author       = {{Merriam-Webster}},
  title        = {Benchmark},
  year         = {2025},
  note         = {Merriam-Webster.com Dictionary. Acesso em: 25 ago. 2025},
  url          = {https://www.merriam-webster.com/dictionary/benchmark}
}

@book{hager2010,
  title={Introduction to high performance computing for scientists and engineers},
  author={Hager, Georg},
  year={2010},
  publisher={Crc Press}
}

@inproceedings{dorta2005,
  title={The OpenMP source code repository},
  author={Dorta, Antonio J and Rodriguez, Casiano and De Sande, Francisco and Gonz{\'a}lez-Escribano, Arturo},
  booktitle={13th Euromicro Conference on Parallel, Distributed and Network-Based Processing},
  pages={244--250},
  year={2005},
  organization={IEEE}
}

@book{terboven2017,
  title={OpenMP: Portable multi-level parallelism on modern systems},
  author={Terboven, Christian and Juckeland, Guido and M{\"u}ller, Matthias and Wellein, Gerhard},
  year={2017},
  publisher={Springer}
}

@book{van2017,
  title={Using OpenMP-The Next Step: Affinity, Accelerators, Tasking, and SIMD},
  author={Van der Pas, Ruud and Stotzer, Eric and Terboven, Christian},
  year={2017},
  publisher={MIT press}
}

@book{kirkhwu2010,
author = {Kirk, David B. and Hwu, Wen-mei W.},
title = {Programming Massively Parallel Processors: A Hands-on Approach},
year = {2010},
isbn = {0123814723},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Multi-core processors are no longer the future of computing-they are the present day reality. A typical mass-produced CPU features multiple processor cores, while a GPU (Graphics Processing Unit) may have hundreds or even thousands of cores. With the rise of multi-core architectures has come the need to teach advanced programmers a new and essential skill: how to program massively parallel processors.Programming Massively Parallel Processors: A Hands-on Approach shows both student and professional alike the basic concepts of parallel programming and GPU architecture. Various techniques for constructing parallel programs are explored in detail. Case studies demonstrate the development process, which begins with computational thinking and ends with effective and efficient parallel programs. Teaches computational thinking and problem-solving techniques that facilitate high-performance parallel computing.Utilizes CUDA (Compute Unified Device Architecture), NVIDIA's software development tool created specifically for massively parallel environments.Shows you how to achieve both high-performance and high-reliability using the CUDA programming model as well as OpenCL.}
}

@article{asanovic2009,
author = {Asanovic, Krste and Bodik, Rastislav and Demmel, James and Keaveny, Tony and Keutzer, Kurt and Kubiatowicz, John and Morgan, Nelson and Patterson, David and Sen, Koushik and Wawrzynek, John and Wessel, David and Yelick, Katherine},
title = {A view of the parallel computing landscape},
year = {2009},
issue_date = {October 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/1562764.1562783},
doi = {10.1145/1562764.1562783},
abstract = {Writing programs that scale with increasing numbers of cores should be as easy as writing programs for sequential computers.},
journal = {Commun. ACM},
month = oct,
pages = {56–67},
numpages = {12}
}

@article{sutter2005,
  title={The free lunch is over: A fundamental turn toward concurrency in software},
  author={Sutter, Herb and others},
  journal={Dr. Dobb’s journal},
  volume={30},
  number={3},
  pages={202--210},
  year={2005}
}

@misc{moore1965,
  title={Cramming more components onto integrated circuits},
  author={Moore, Gordon E},
  year={1965}
}

@inproceedings{amdahl1967,
author = {Amdahl, Gene M.},
title = {Validity of the single processor approach to achieving large scale computing capabilities},
year = {1967},
isbn = {9781450378956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1465482.1465560},
doi = {10.1145/1465482.1465560},
abstract = {For over a decade prophets have voiced the contention that the organization of a single computer has reached its limits and that truly significant advances can be made only by interconnection of a multiplicity of computers in such a manner as to permit cooperative solution. Variously the proper direction has been pointed out as general purpose computers with a generalized interconnection of memories, or as specialized computers with geometrically related memory interconnections and controlled by one or more instruction streams.},
booktitle = {Proceedings of the April 18-20, 1967, Spring Joint Computer Conference},
pages = {483–485},
numpages = {3},
location = {Atlantic City, New Jersey},
series = {AFIPS '67 (Spring)}
}

@inproceedings{frigoleisersonrandall1998,
author = {Frigo, Matteo and Leiserson, Charles E. and Randall, Keith H.},
title = {The implementation of the Cilk-5 multithreaded language},
year = {1998},
isbn = {0897919874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/277650.277725},
doi = {10.1145/277650.277725},
abstract = {The fifth release of the multithreaded language Cilk uses a provably good "work-stealing" scheduling algorithm similar to the first system, but the language has been completely redesigned and the runtime system completely reengineered. The efficiency of the new implementation was aided by a clear strategy that arose from a theoretical analysis of the scheduling algorithm: concentrate on minimizing overheads that contribute to the work, even at the expense of overheads that contribute to the critical path. Although it may seem counterintuitive to move overheads onto the critical path, this "work-first" principle has led to a portable Cilk-5 implementation in which the typical cost of spawning a parallel thread is only between 2 and 6 times the cost of a C function call on a variety of contemporary machines. Many Cilk programs run on one processor with virtually no degradation compared to equivalent C programs. This paper describes how the work-first principle was exploited in the design of Cilk-5's compiler and its runtime system. In particular, we present Cilk-5's novel "two-clone" compilation strategy and its Dijkstra-like mutual-exclusion protocol for implementing the ready deque in the work-stealing scheduler.},
booktitle = {Proceedings of the ACM SIGPLAN 1998 Conference on Programming Language Design and Implementation},
pages = {212–223},
numpages = {12},
keywords = {critical path, multithreading, parallel computing, programming language, runtime system, work},
location = {Montreal, Quebec, Canada},
series = {PLDI '98}
}

@article{gotogeijn2008,
author = {Goto, Kazushige and Geijn, Robert A. van de},
title = {Anatomy of high-performance matrix multiplication},
year = {2008},
issue_date = {May 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {3},
issn = {0098-3500},
url = {https://doi.org/10.1145/1356052.1356053},
doi = {10.1145/1356052.1356053},
abstract = {We present the basic principles that underlie the high-performance implementation of the matrix-matrix multiplication that is part of the widely used GotoBLAS library. Design decisions are justified by successively refining a model of architectures with multilevel memories. A simple but effective algorithm for executing this operation results. Implementations on a broad selection of architectures are shown to achieve near-peak performance.},
journal = {ACM Trans. Math. Softw.},
month = may,
articleno = {12},
numpages = {25},
keywords = {Linear algebra, basic linear algebra subprogrms, matrix multiplication}
}

@article{grama1993isoefficiency,
  author    = {Grama, Ananth and Gupta, Anshul and Kumar, Vipin},
  title     = {Isoefficiency: measuring the scalability of parallel algorithms and architectures},
  journal   = {IEEE Parallel \& Distributed Technology},
  volume    = {1},
  number    = {3},
  pages     = {12--21},
  year      = {1993}
}

@misc{umd2024isoefficiency,
  author       = {University of Maryland},
  title        = {Isoefficiency is a metric for scalability},
  year         = {2024},
  howpublished = {\url{https://www.cs.umd.edu/class/spring2024/cmsc416/scribe-notes/02-22-2024-2.pdf}},
  note         = {Acesso em: 19 ago. 2025}
}

@misc{purdue2023isoefficiency,
  author       = {Purdue University},
  title        = {Isoefficiency analysis allows us},
  year         = {2023},
  howpublished = {\url{https://engineering.purdue.edu/~smidkiff/ece563/slides/Isoefficiency.pdf}},
  note         = {Acesso em: 19 ago. 2025}
}

@book{kumargramagupta1994,
author = {Kumar, Vipin and Grama, Ananth and Gupta, Anshul and Karypis, George},
title = {Introduction to parallel computing: design and analysis of algorithms},
year = {1994},
isbn = {0805331700},
publisher = {Benjamin-Cummings Publishing Co., Inc.},
address = {USA}
}

@article{gustafson1988,
author = {Gustafson, John L.},
title = {Reevaluating Amdahl's law},
year = {1988},
issue_date = {May 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/42411.42415},
doi = {10.1145/42411.42415},
journal = {Commun. ACM},
month = may,
pages = {532–533},
numpages = {2}
}

@book{hennessypatterson2017,
author = {Hennessy, John L. and Patterson, David A.},
title = {Computer Architecture, Sixth Edition: A Quantitative Approach},
year = {2017},
isbn = {0128119055},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {6th},
abstract = {Computer Architecture: A Quantitative Approach, Sixth Edition has been considered essential reading by instructors, students and practitioners of computer design for over 20 years. The sixth edition of this classic textbook is fully revised with the latest developments in processor and system architecture. It now features examples from the RISC-V (RISC Five) instruction set architecture, a modern RISC instruction set developed and designed to be a free and openly adoptable standard. It also includes a new chapter on domain-specific architectures and an updated chapter on warehouse-scale computing that features the first public information on Google's newest WSC. True to its original mission of demystifying computer architecture, this edition continues the longstanding tradition of focusing on areas where the most exciting computing innovation is happening, while always keeping an emphasis on good engineering design. Includes a new chapter on domain-specific architectures, explaining how they are the only path forward for improved performance and energy efficiency given the end of Moores Law and Dennard scaling Features the first publication of several DSAs from industry Features extensive updates to the chapter on warehouse-scale computing, with the first public information on the newest Google WSC Offers updates to other chapters including new material dealing with the use of stacked DRAM; data on the performance of new NVIDIA Pascal GPU vs. new AVX-512 Intel Skylake CPU; and extensive additions to content covering multicore architecture and organization Includes "Putting It All Together" sections near the end of every chapter, providing real-world technology examples that demonstrate the principles covered in each chapter Includes review appendices in the printed text and additional reference appendices available online Includes updated and improved case studies and exercises}
}

@book{mcoolreindersrobison2012,
author = {McCool, Michael and Reinders, James and Robison, Arch},
title = {Structured Parallel Programming: Patterns for Efficient Computation},
year = {2012},
isbn = {9780123914439},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Programming is now parallel programming. Much as structured programming revolutionized traditional serial programming decades ago, a new kind of structured programming, based on patterns, is relevant to parallel programming today. Parallel computing experts and industry insiders Michael McCool, Arch Robison, and James Reinders describe how to design and implement maintainable and efficient parallel algorithms using a pattern-based approach. They present both theory and practice, and give detailed concrete examples using multiple programming models. Examples are primarily given using two of the most popular and cutting edge programming models for parallel programming: Threading Building Blocks, and Cilk Plus. These architecture-independent models enable easy integration into existing applications, preserve investments in existing code, and speed the development of parallel applications. Examples from realistic contexts illustrate patterns and themes in parallel algorithm design that are widely applicable regardless of implementation technology. The patterns-based approach offers structure and insight that developers can apply to a variety of parallel programming models Develops a composable, structured, scalable, and machine-independent approach to parallel computing Includes detailed examples in both Cilk Plus and the latest Threading Building Blocks, which support a wide variety of computers Table of Contents 1. Introduction 2. Map 3. Collectives 4. Data reorganization 5. Fork-join 6. Examples 7. Further Reading}
}

@inproceedings{nickolletal2008,
author = {Nickolls, John and Buck, Ian and Garland, Michael and Skadron, Kevin},
title = {Scalable parallel programming with CUDA},
year = {2008},
isbn = {9781450378451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1401132.1401152},
doi = {10.1145/1401132.1401152},
abstract = {Is CUDA the parallel programming model that application developers have been waiting for?},
booktitle = {ACM SIGGRAPH 2008 Classes},
articleno = {16},
numpages = {14},
location = {Los Angeles, California},
series = {SIGGRAPH '08}
}

@book{quinn2003,
author = {Quinn, Michael J.},
title = {Parallel Programming in C with MPI and OpenMP},
year = {2003},
isbn = {0071232656},
publisher = {McGraw-Hill Education Group}
}

@article{gropp1996,
  title={A high-performance, portable implementation of the MPI message passing interface standard},
  author={Gropp, William and Lusk, Ewing and Skjellum, Anthony},
  journal={Parallel computing},
  volume={22},
  number={6},
  pages={789--828},
  year={1996},
  publisher={Elsevier}
}

@inproceedings{rabenseifner2009,
  title={Hybrid MPI/OpenMP parallel programming on clusters of multi-core SMP nodes},
  author={Rabenseifner, Rolf and Hager, Georg and Jost, Gabriele},
  booktitle={2009 17th Euromicro International Conference on Parallel, Distributed and Network-based Processing},
  pages={427--436},
  year={2009},
  organization={IEEE}
}

@article{thakurrabenseifnergropp2005,
author = {Thakur, Rajeev and Rabenseifner, Rolf and Gropp, William},
title = {Optimization of Collective Communication Operations in MPICH},
year = {2005},
issue_date = {February  2005},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {19},
number = {1},
issn = {1094-3420},
url = {https://doi.org/10.1177/1094342005051521},
doi = {10.1177/1094342005051521},
abstract = {We describe our work on improving the performance of collective communication operations in MPICH for clusters connected by switched networks. For each collective operation, we use multiple algorithms depending on the message size, with the goal of minimizing latency for short messages and minimizing bandwidth use for long messages. Although we have implemented new algorithms for all MPI Message Passing Interface collective operations, because of limited space we describe only the algorithms for allgather, broadcast, all-to-all, reduce-scatter, reduce, and allreduce. Performance results on a Myrinet-connected Linux cluster and an IBM SP indicate that, in all cases, the new algorithms significantly outperform the old algorithms used in MPICH on the Myrinet cluster, and, in many cases, they outperform the algorithms used in IBM's MPI on the SP. We also explore in further detail the optimization of two of the most commonly used collective operations, allreduce and reduce, particularly for long messages and nonpower-of-two numbers of processes. The optimized algorithms for these operations perform several times better than the native algorithms on a Myrinet cluster, IBM SP, and Cray T3E. Our results indicate that to achieve the best performance for a collective communication operation, one needs to use a number of different algorithms and select the right algorithm for a particular message size and number of processes.},
journal = {Int. J. High Perform. Comput. Appl.},
month = feb,
pages = {49–66},
numpages = {18},
keywords = {Collective communication, MPI, message passing, reduction}
}

@article{owens2008,
  title={GPU computing},
  author={Owens, John D and Houston, Mike and Luebke, David and Green, Simon and Stone, John E and Phillips, James C},
  journal={Proceedings of the IEEE},
  volume={96},
  number={5},
  pages={879--899},
  year={2008},
  publisher={IEEE}
}

@article{valiant1990,
author = {Valiant, Leslie G.},
title = {A bridging model for parallel computation},
year = {1990},
issue_date = {Aug. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/79173.79181},
doi = {10.1145/79173.79181},
abstract = {The success of the von Neumann model of sequential computation is attributable to the fact that it is an efficient bridge between software and hardware: high-level languages can be efficiently compiled on to this model; yet it can be effeciently implemented in hardware. The author argues that an analogous bridge between software and hardware in required for parallel computation if that is to become as widely used. This article introduces the bulk-synchronous parallel (BSP) model as a candidate for this role, and gives results quantifying its efficiency both in implementing high-level language features and algorithms, as well as in being implemented in hardware.},
journal = {Commun. ACM},
month = aug,
pages = {103–111},
numpages = {9}
}

@article{williamswatermanpatterson2009,
author = {Williams, Samuel and Waterman, Andrew and Patterson, David},
title = {Roofline: an insightful visual performance model for multicore architectures},
year = {2009},
issue_date = {April 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/1498765.1498785},
doi = {10.1145/1498765.1498785},
abstract = {The Roofline model offers insight on how to improve the performance of software and hardware.},
journal = {Commun. ACM},
month = apr,
pages = {65–76},
numpages = {12}
}

@article{blumoleiserson1999,
author = {Blumofe, Robert D. and Leiserson, Charles E.},
title = {Scheduling multithreaded computations by work stealing},
year = {1999},
issue_date = {Sept. 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {5},
issn = {0004-5411},
url = {https://doi.org/10.1145/324133.324234},
doi = {10.1145/324133.324234},
abstract = {This paper studies the problem of efficiently schedulling fully strict (i.e., well-structured) multithreaded computations on parallel computers. A popular and practical method of scheduling this kind of dynamic MIMD-style computation is “work stealing,” in which processors needing work steal computational threads from other processors. In this paper, we give the first provably good work-stealing scheduler for multithreaded computations with dependencies.Specifically, our analysis shows that the expected time to execute a fully strict computation on P processors using our work-stealing scheduler is T1/P + O(T ∞ , where T1 is the minimum serial execution time of the multithreaded computation and (T ∞ is the minimum execution time with an infinite number of processors. Moreover, the space required by the execution is at most S1P, where S1 is the minimum serial space requirement. We also show that the expected total communication of the algorithm is at most O(PT ∞( 1 + nd)Smax), where Smax is the size of the largest activation record of any thread and nd is the maximum number of times that any thread synchronizes with its parent. This communication bound justifies the folk wisdom that work-stealing schedulers are more communication efficient than their work-sharing counterparts. All three of these bounds are existentially optimal to within a constant factor.},
journal = {J. ACM},
month = sep,
pages = {720–748},
numpages = {29},
keywords = {work stealing, thread scheduling, randomized algorithm, multithreading, multiprocessor, critical-path length}
}

@article{brent1974,
author = {Brent, Richard P.},
title = {The Parallel Evaluation of General Arithmetic Expressions},
year = {1974},
issue_date = {April 1974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0004-5411},
url = {https://doi.org/10.1145/321812.321815},
doi = {10.1145/321812.321815},
abstract = {It is shown that arithmetic expressions with n ≥ 1 variables and constants; operations of addition, multiplication, and division; and any depth of parenthesis nesting can be evaluated in time 4 log2n + 10(n - 1)/p using p ≥ 1 processors which can independently perform arithmetic operations in unit time. This bound is within a constant factor of the best possible. A sharper result is given for expressions without the division operation, and the question of numerical stability is discussed.},
journal = {J. ACM},
month = apr,
pages = {201–206},
numpages = {6}
}

@article{culleretal1993,
author = {Culler, David and Karp, Richard and Patterson, David and Sahay, Abhijit and Schauser, Klaus Erik and Santos, Eunice and Subramonian, Ramesh and von Eicken, Thorsten},
title = {LogP: towards a realistic model of parallel computation},
year = {1993},
issue_date = {July 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {7},
issn = {0362-1340},
url = {https://doi.org/10.1145/173284.155333},
doi = {10.1145/173284.155333},
abstract = {A vast body of theoretical research has focused either on overly simplistic models of parallel computation, notably the PRAM, or overly specific models that have few representatives in the real world. Both kinds of models encourage exploitation of formal loopholes, rather than rewarding development of techniques that yield performance across a range of current and future parallel machines. This paper offers a new parallel machine model, called LogP, that reflects the critical technology trends underlying parallel computers. it is intended to serve as a basis for developing fast, portable parallel algorithms and to offer guidelines to machine designers. Such a model must strike a balance between detail and simplicity in order to reveal important bottlenecks without making analysis of interesting problems intractable. The model is based on four parameters that specify abstractly the computing bandwidth, the communication bandwidth, the communication delay, and the efficiency of coupling communication and computation. Portable parallel algorithms typically adapt to the machine configuration, in terms of these parameters. The utility of the model is demonstrated through examples that are implemented on the CM-5.},
journal = {SIGPLAN Not.},
month = jul,
pages = {1–12},
numpages = {12},
keywords = {PRAM, complexity analysis, massively parallel processors, parallel algorithms, parallel models}
}

@inproceedings{balaji2011,
  title={MPI on a million processors},
  author={Balaji, Pavan and Buntinas, Darius and Goodell, David and Gropp, William and Thakur, Rajeev},
  booktitle={Proceedings of the 16th ACM SIGPLAN symposium on Principles and practice of parallel programming},
  pages={20--30},
  year={2011}
}

@article{dagummenon1998,
author = {Dagum, Leonardo and Menon, Ramesh},
title = {OpenMP: An Industry-Standard API for Shared-Memory Programming},
year = {1998},
issue_date = {January 1998},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {5},
number = {1},
issn = {1070-9924},
url = {https://doi.org/10.1109/99.660313},
doi = {10.1109/99.660313},
abstract = {The authors present a new way to achieve scalability in parallel software with OpenMP, their portable alternative to message passing. They discuss its capabilities through specific examples and comparisons with other standard parallel programming models.},
journal = {IEEE Comput. Sci. Eng.},
month = jan,
pages = {46–55},
numpages = {10}
}

@article{polychronopouloskuck1987,
author = {Polychronopoulos, C. D. and Kuck, D. J.},
title = {Guided self-scheduling: A practical scheduling scheme for parallel supercomputers},
year = {1987},
issue_date = {Dec. 1987},
publisher = {IEEE Computer Society},
address = {USA},
volume = {36},
number = {12},
issn = {0018-9340},
url = {https://doi.org/10.1109/TC.1987.5009495},
doi = {10.1109/TC.1987.5009495},
abstract = {This paper proposes guided self-scheduling, a new approach for scheduling arbitrarily nested parallel program loops on shared memory multiprocessor systems. Utilizing loop parallelism is clearly most crucial in achieving high system and program performance. Because of its simplicity, guided self-scheduling is particularly suited for implementation on real parallel machines. This method achieves simultaneously the two most important objectives: load balancing and very low synchronization overhead. For certain types of loops we show analytically that guided self-scheduling uses minimal overhead and achieves optimal schedules. Two other interesting properties of this method are its insensitivity to the initial processor configuration (in time) and its parameterized nature which allows us to tune it for different systems. Finally we discuss experimental results that clearly show the advantage of guided self-scheduling over the most widely known dynamic methods.},
journal = {IEEE Trans. Comput.},
month = dec,
pages = {1425–1439},
numpages = {15},
keywords = {synchronization, self-scheduling, run-time overhead, parallel supercomputers, parallel loops, Parallel Fortran programs}
}

@book{hwang1993,
  title={Advanced computer architecture: parallelism, scalability, programmability},
  author={Hwang, Kai and Jotwani, Naresh},
  volume={199},
  year={1993},
  publisher={McGraw-Hill New York}
}

@inproceedings{baileyetal1991,
author = {Bailey, D. H. and Barszcz, E. and Barton, J. T. and Browning, D. S. and Carter, R. L. and Dagum, L. and Fatoohi, R. A. and Frederickson, P. O. and Lasinski, T. A. and Schreiber, R. S. and Simon, H. D. and Venkatakrishnan, V. and Weeratunga, S. K.},
title = {The NAS parallel benchmarks—summary and preliminary results},
year = {1991},
isbn = {0897914597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/125826.125925},
doi = {10.1145/125826.125925},
booktitle = {Proceedings of the 1991 ACM/IEEE Conference on Supercomputing},
pages = {158–165},
numpages = {8},
location = {Albuquerque, New Mexico, USA},
series = {Supercomputing '91}
}


@inproceedings{chandrasekaran2017,
  title={OpenMP: An evolving standard for parallel programming},
  author={Chandrasekaran, Sunita and Chapman, Barbara},
  booktitle={International Conference on High Performance Computing},
  pages={1--15},
  year={2017},
  organization={Springer}
}

@techreport{bailey1995,
  title={The NAS parallel benchmarks 2.0},
  author={Bailey, David and Harris, Tim and Saphir, William and Van Der Wijngaart, Rob and Woo, Alex and Yarrow, Maurice},
  year={1995},
  institution={Technical Report NAS-95-020, NASA Ames Research Center}
}

@book{groppluskskjellum2014,
author = {Gropp, William and Lusk, Ewing and Skjellum, Anthony},
title = {Using MPI: Portable Parallel Programming with the Message-Passing Interface},
year = {2014},
isbn = {0262527391},
publisher = {The MIT Press},
abstract = {This book offers a thoroughly updated guide to the MPI (Message-Passing Interface) standard library for writing programs for parallel computers. Since the publication of the previous edition of Using MPI, parallel computing has become mainstream. Today, applications run on computers with millions of processors; multiple processors sharing memory and multicore processors with multiple hardware threads per core are common. The MPI-3 Forum recently brought the MPI standard up to date with respect to developments in hardware capabilities, core language evolution, the needs of applications, and experience gained over the years by vendors, implementers, and users. This third edition of Using MPI reflects these changes in both text and example code. The book takes an informal, tutorial approach, introducing each concept through easy-to-understand examples, including actual code in C and Fortran. Topics include using MPI in simple programs, virtual topologies, MPI datatypes, parallel libraries, and a comparison of MPI with sockets. For the third edition, example code has been brought up to date; applications have been updated; and references reflect the recent attention MPI has received in the literature. A companion volume, Using Advanced MPI, covers more advanced topics, including hybrid programming and coping with large data.}
}

@book{tanenbaumsteen2006,
author = {Tanenbaum, Andrew S. and Steen, Maarten van},
title = {Distributed Systems: Principles and Paradigms (2nd Edition)},
year = {2006},
isbn = {0132392275},
publisher = {Prentice-Hall, Inc.},
address = {USA}
}

@article{hillmarty2008,
author = {Hill, Mark D. and Marty, Michael R.},
title = {Amdahl's Law in the Multicore Era},
year = {2008},
issue_date = {July 2008},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {41},
number = {7},
issn = {0018-9162},
url = {https://doi.org/10.1109/MC.2008.209},
doi = {10.1109/MC.2008.209},
abstract = {Augmenting Amdahl's law with a corollary for multicore hardware makes it relevant to future generations of chips with multiple processor cores. Obtaining optimal multicore performance will require further research in both extracting more parallelism and making sequential cores faster.},
journal = {Computer},
month = jul,
pages = {33–38},
numpages = {6},
keywords = {multicore chips, chip multiprocessors (CMPs), Amdahl's law}
}

@inproceedings{bieniaetal2008,
author = {Bienia, Christian and Kumar, Sanjeev and Singh, Jaswinder Pal and Li, Kai},
title = {The PARSEC benchmark suite: characterization and architectural implications},
year = {2008},
isbn = {9781605582825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1454115.1454128},
doi = {10.1145/1454115.1454128},
abstract = {This paper presents and characterizes the Princeton Application Repository for Shared-Memory Computers (PARSEC), a benchmark suite for studies of Chip-Multiprocessors (CMPs). Previous available benchmarks for multiprocessors have focused on high-performance computing applications and used a limited number of synchronization methods. PARSEC includes emerging applications in recognition, mining and synthesis (RMS) as well as systems applications which mimic large-scale multithreaded commercial programs. Our characterization shows that the benchmark suite covers a wide spectrum of working sets, locality, data sharing, synchronization and off-chip traffic. The benchmark suite has been made available to the public.},
booktitle = {Proceedings of the 17th International Conference on Parallel Architectures and Compilation Techniques},
pages = {72–81},
numpages = {10},
keywords = {shared-memory computers, performance measurement, multithreading, benchmark suite},
location = {Toronto, Ontario, Canada},
series = {PACT '08}
}

@article{henning2006,
  title={SPEC CPU2006 benchmark descriptions},
  author={Henning, John L},
  journal={ACM SIGARCH Computer Architecture News},
  volume={34},
  number={4},
  pages={1--17},
  year={2006},
  publisher={ACM New York, NY, USA}
}

@article{stratton2012,
  title={Parboil: A revised benchmark suite for scientific and commercial throughput computing},
  author={Stratton, John A and Rodrigues, Christopher and Sung, I-Jui and Obeid, Nady and Chang, Li-Wen and Anssari, Nasser and Liu, Geng Daniel and Hwu, Wen-mei W},
  journal={Center for Reliable and High-Performance Computing},
  volume={127},
  number={7.2},
  year={2012}
}

@inproceedings{che2009,
  title={Rodinia: A benchmark suite for heterogeneous computing},
  author={Che, Shuai and Boyer, Michael and Meng, Jiayuan and Tarjan, David and Sheaffer, Jeremy W and Lee, Sang-Ha and Skadron, Kevin},
  booktitle={2009 IEEE international symposium on workload characterization (IISWC)},
  pages={44--54},
  year={2009},
  organization={Ieee}
}

@inproceedings{hoefler2010,
  title={Characterizing the influence of system noise on large-scale applications by simulation},
  author={Hoefler, Torsten and Schneider, Timo and Lumsdaine, Andrew},
  booktitle={SC'10: Proceedings of the 2010 ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--11},
  year={2010},
  organization={IEEE}
}

@techreport{dongarrameuerstrohmaier1999,
author = {Dongarra, Jack J. and Meuer, Hans-Werner and Strohmaier, Erich},
title = {Top500 Supercomputer Sites - 13th edition},
year = {1999},
publisher = {University of Tennessee},
address = {USA}
}

@article{jinma2000,
author = {Jin, H. and MA, Frumkin},
year = {2000},
month = {05},
pages = {},
title = {The OpenMP Implementation of NAS Parallel Benchmarks and Its Performance}
}

@article{jin1999,
  title={The OpenMP implementation of NAS parallel benchmarks and its performance},
  author={Jin, Hao-Qiang and Frumkin, Michael and Yan, Jerry},
  year={1999}
}

@article{shalf2020,
  title={The future of computing beyond Moore’s Law},
  author={Shalf, John},
  journal={Philosophical Transactions of the Royal Society A},
  volume={378},
  number={2166},
  pages={20190061},
  year={2020},
  publisher={The Royal Society Publishing}
}

@article{zhang2019,
  title={Machine learning on GPUs: a comprehensive survey},
  author={Zhang, Ce and others},
  journal={ACM Computing Surveys},
  volume={52},
  number={3},
  pages={1--37},
  year={2019},
  publisher={ACM}
}

@article{dongarra1995,
  title={A set of level 3 basic linear algebra communication subprograms},
  author={Dongarra, Jack and Duff, Iain S and Sorensen, Danny C and Van der Vorst, Henk A},
  journal={ACM Transactions on Mathematical Software},
  volume={16},
  number={1},
  pages={1--17},
  year={1995}
}

@article{dongarraetal2011,
author = {Dongarra, Jack and Beckman, Pete and Moore, Terry and Aerts, Patrick and Aloisio, Giovanni and Andre, Jean-Claude and Barkai, David and Berthou, Jean-Yves and Boku, Taisuke and Braunschweig, Bertrand and Cappello, Franck and Chapman, Barbara and Xuebin Chi and Choudhary, Alok and Dosanjh, Sudip and Dunning, Thom and Fiore, Sandro and Geist, Al and Gropp, Bill and Harrison, Robert and Hereld, Mark and Heroux, Michael and Hoisie, Adolfy and Hotta, Koh and Zhong Jin and Ishikawa, Yutaka and Johnson, Fred and Kale, Sanjay and Kenway, Richard and Keyes, David and Kramer, Bill and Labarta, Jesus and Lichnewsky, Alain and Lippert, Thomas and Lucas, Bob and Maccabe, Barney and Matsuoka, Satoshi and Messina, Paul and Michielse, Peter and Mohr, Bernd and Mueller, Matthias S. and Nagel, Wolfgang E. and Nakashima, Hiroshi and Papka, Michael E and Reed, Dan and Sato, Mitsuhisa and Seidel, Ed and Shalf, John and Skinner, David and Snir, Marc and Sterling, Thomas and Stevens, Rick and Streitz, Fred and Sugar, Bob and Sumimoto, Shinji and Tang, William and Taylor, John and Thakur, Rajeev and Trefethen, Anne and Valero, Mateo and Van Der Steen, Aad and Vetter, Jeffrey and Williams, Peg and Wisniewski, Robert and Yelick, Kathy},
title = {The International Exascale Software Project roadmap},
year = {2011},
issue_date = {February  2011},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {25},
number = {1},
issn = {1094-3420},
url = {https://doi.org/10.1177/1094342010391989},
doi = {10.1177/1094342010391989},
abstract = {Over the last 20 years, the open-source community has provided more and more software on which the world\^{a} s high-performance computing systems depend for performance and productivity. The community has invested millions of dollars and years of effort to build key components. However, although the investments in these separate software elements have been tremendously valuable, a great deal of productivity has also been lost because of the lack of planning, coordination, and key integration of technologies necessary to make them work together smoothly and efficiently, both within individual petascale systems and between different systems. It seems clear that this completely uncoordinated development model will not provide the software needed to support the unprecedented parallelism required for peta/ exascale computation on millions of cores, or the flexibility required to exploit new hardware models and features, such as transactional memory, speculative execution, and graphics processing units. This report describes the work of the community to prepare for the challenges of exascale computing, ultimately combing their efforts in a coordinated International Exascale Software Project.},
journal = {Int. J. High Perform. Comput. Appl.},
month = feb,
pages = {3–60},
numpages = {58},
keywords = {exascale computing, high-performance computing, software stack}
}

@article{ricogallegoetal2019,
author = {Rico-Gallego, Juan A. and D\'{\i}az-Mart\'{\i}n, Juan C. and Manumachu, Ravi Reddy and Lastovetsky, Alexey L.},
title = {A Survey of Communication Performance Models for High-Performance Computing},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3284358},
doi = {10.1145/3284358},
abstract = {This survey aims to present the state of the art in analytic communication performance models, providing sufficiently detailed descriptions of particularly noteworthy efforts. Modeling the cost of communications in computer clusters is an important and challenging problem. It provides insights into the design of the communication pattern of parallel scientific applications and mathematical kernels and sets a clear ground for optimization of their deployment in the increasingly complex high-performance computing infrastructure. The survey provides background information on how different performance models represent the underlying platform and shows the evolution of these models over time from early clusters of single-core processors to present-day multi-core and heterogeneous platforms. Prospective directions for future research in the area of analytic communication performance modeling conclude the survey.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {126},
numpages = {36},
keywords = {Communication performance models, analytic modeling, communication performance, high-performance computing}
}

@article{mittalvetter2014,
author = {Mittal, Sparsh and Vetter, Jeffrey S.},
title = {A Survey of Methods for Analyzing and Improving GPU Energy Efficiency},
year = {2014},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/2636342},
doi = {10.1145/2636342},
abstract = {Recent years have witnessed phenomenal growth in the computational capabilities and applications of GPUs. However, this trend has also led to a dramatic increase in their power consumption. This article surveys research works on analyzing and improving energy efficiency of GPUs. It also provides a classification of these techniques on the basis of their main research idea. Further, it attempts to synthesize research works that compare the energy efficiency of GPUs with other computing systems (e.g., FPGAs and CPUs). The aim of this survey is to provide researchers with knowledge of the state of the art in GPU power management and motivate them to architect highly energy-efficient GPUs of tomorrow.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {19},
numpages = {23},
keywords = {GPU (graphics-processing unit), architecture techniques, energy efficiency, energy saving, green computing, power management, power model}
}

@article{karp1990,
author = {Karp, Alan H. and Flatt, Horace P.},
title = {Measuring parallel processor performance},
year = {1990},
issue_date = {May 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/78607.78614},
doi = {10.1145/78607.78614},
abstract = {The Karp-Flatt metric is a simple way to measure the performance of parallel processors that is independent of the number of processors used. The metric is based on the measured speedup and provides an estimate of the serial fraction of the code. By measuring the speedup on successively larger numbers of processors, one can determine whether the code is likely to benefit from using more processors. The metric is shown to be consistent with Amdahl's Law.},
journal = {Commun. ACM},
month = may,
pages = {539–543},
numpages = {5},
keywords = {parallel processing, performance measurement, speedup}
}

@article{eager1989,
author = {Eager, Derek L. and Zahorjan, John and Lazowska, Edward D.},
title = {Speedup versus efficiency in parallel systems},
year = {1989},
issue_date = {March 1989},
publisher = {IEEE Computer Society},
address = {USA},
volume = {38},
number = {3},
issn = {0018-9340},
url = {https://doi.org/10.1109/12.21127},
doi = {10.1109/12.21127},
abstract = {A variety of perspectives on speedup versus efficiency in parallel systems are discussed. Several examples are presented to illustrate that "superlinear speedup" is not paradoxical, and that efficiency can exceed 100\%. Simple analytic models of the memory hierarchy are used to demonstrate that cache effects can cause superlinear speedup for realistic problem sizes. The authors comment on various speedup anomalies that have been reported in the literature and note that care must be taken in interpreting such anomalies.},
journal = {IEEE Trans. Comput.},
month = mar,
pages = {408–423},
numpages = {16},
keywords = {cache memory, memory hierarchy, parallel processing, performance evaluation, speedup anomalies, superlinear speedup}
}

@inproceedings{sundergurumurthi2001,
author = {Sunder, V. S. and Gurumurthi, S. and Karakoyunlu, Cevdet and Liu, X. and Stan, M. R. and Vijaykrishnan, N. and Irwin, M. J. and Sivasubramaniam, A.},
title = {Exploiting inherent redundancy to improve tolerance to transients in chip multiprocessors},
year = {2001},
isbn = {0769510981},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICCD.2001.955059},
doi = {10.1109/ICCD.2001.955059},
abstract = {Chip multiprocessors (CMPs) integrate multiple processor cores on a single die. When executing multithreaded workloads on CMPs, it is common for threads to exhibit redundancy - multiple threads may execute similar or identical operations. In this work, we explore techniques to exploit this inherent redundancy to improve tolerance to transient faults. We propose two schemes: 1) a hardware-based scheme that exploits thread-level redundancy (TLR) by monitoring instruction streams of different threads for similar operations, and 2) a software-based scheme that uses compiler support to identify and tag redundant operations across threads. Our evaluation shows that both schemes can significantly improve the detection coverage of transient faults with modest performance and power overheads.},
booktitle = {Proceedings of the 2001 IEEE International Conference on Computer Design: VLSI in Computers and Processors},
pages = {356–363},
numpages = {8},
keywords = {chip multiprocessors, fault tolerance, redundancy, transient faults},
series = {ICCD '01}
}

@book{wilkinson2009,
author = {Wilkinson, Barry and Allen, Michael},
title = {Parallel Programming: Techniques and Applications Using Networked Workstations and Parallel Computers},
year = {2009},
isbn = {0131405632},
publisher = {Pearson Prentice Hall},
edition = {2nd},
abstract = {This book provides a comprehensive introduction to parallel programming using a variety of programming models and platforms. It covers fundamental concepts such as partitioning, communication, and synchronization, and discusses practical implementation issues including load balancing and performance optimization. The text includes numerous examples and exercises to reinforce key concepts.}
}

@article{suleman2009,
author = {Suleman, M. Aater and Mutlu, Onur and Joao, Jose A. and Khubaib and Patt, Yale N.},
title = {Data marshaling for multi-core architectures},
year = {2009},
issue_date = {March 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {0163-5964},
url = {https://doi.org/10.1145/1508244.1508276},
doi = {10.1145/1508244.1508276},
abstract = {Multi-core processors are becoming prevalent as transistor density continues to increase. However, parallel programming remains difficult due to communication and synchronization overheads. This paper proposes data marshaling, a technique that reduces these overheads by optimizing data movement patterns across cores. The authors demonstrate that careful orchestration of data transfers can significantly improve the performance of parallel applications on multi-core architectures.},
journal = {SIGARCH Comput. Archit. News},
month = mar,
pages = {441–450},
numpages = {10},
keywords = {cache coherence, data movement, multi-core processors, parallel programming}
}

@inproceedings{leebrooks2006,
author = {Lee, Victor W. and Brooks, David},
title = {Accurate and efficient regression modeling for microarchitectural performance and power prediction},
year = {2006},
isbn = {1595934510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1168857.1168881},
doi = {10.1145/1168857.1168881},
abstract = {This paper presents regression-based models for predicting processor performance and power consumption. The models use hardware performance counters as predictors and employ machine learning techniques to build accurate models with minimal training overhead. The approach is validated on multiple processor architectures and shown to achieve high prediction accuracy.},
booktitle = {Proceedings of the 12th International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {185–194},
numpages = {10},
keywords = {machine learning, performance modeling, power modeling, regression analysis},
location = {San Jose, California, USA},
series = {ASPLOS XII}
}

@article{ghaisasmahajan2017,
author = {Ghaisa, Nilay and Mahajan, Ruchir},
title = {Energy efficiency and thermal management in modern microprocessors},
year = {2017},
issue_date = {September 2017},
publisher = {IEEE},
volume = {10},
number = {3},
pages = {8--13},
numpages = {6},
keywords = {energy efficiency, thermal management, microprocessors, power consumption}
}

@inproceedings{hammarchersson2014,
author = {Hammarcher, Carl and Vranesic, Zvonko and Zaky, Safwat and Manjikian, Naraig},
title = {Computer Organization and Embedded Systems},
year = {2014},
publisher = {McGraw-Hill Education},
edition = {6th},
abstract = {This textbook provides a comprehensive introduction to computer organization, covering topics from basic digital logic to advanced processor architectures. It emphasizes practical design considerations and includes discussions on embedded systems, memory hierarchies, and input/output systems.}
}

@article{snavely2000,
author = {Snavely, Allan and Carrington, Laura and Wolter, Nicole and Labarta, Jesus and Badia, Rosa and Purkayastha, Anirban},
title = {A framework for performance modeling and prediction},
year = {2000},
publisher = {IEEE},
url = {https://doi.org/10.1109/SC.2002.10010},
doi = {10.1109/SC.2002.10010},
abstract = {This paper presents a framework for modeling and predicting the performance of parallel applications on high-performance computing systems. The framework combines analytical models with empirical measurements to provide accurate predictions of application performance across different system configurations.},
journal = {Proceedings of the ACM/IEEE SC2002 Conference},
pages = {21--21},
keywords = {performance modeling, performance prediction, parallel computing}
}

@article{dennard1974,
author = {Dennard, Robert H. and Gaensslen, Fritz H. and Yu, Hwa-Nien and Rideout, V. Leo and Bassous, Ernest and LeBlanc, Andre R.},
title = {Design of ion-implanted MOSFET's with very small physical dimensions},
year = {1974},
issue_date = {October 1974},
publisher = {IEEE},
volume = {9},
number = {5},
pages = {256--268},
numpages = {13},
journal = {IEEE Journal of Solid-State Circuits},
keywords = {MOSFET scaling, Dennard scaling, power density, transistor miniaturization}
}

@inproceedings{levinthal2009,
author = {Levinthal, Adam},
title = {Performance analysis guide for Intel Core i7 processor and Intel Xeon 5500 processors},
year = {2009},
publisher = {Intel Corporation},
abstract = {This guide provides detailed information on performance analysis for Intel's Core i7 and Xeon 5500 processor families. It covers architecture-specific features, performance monitoring capabilities, and optimization strategies for achieving maximum performance on these platforms.}
}

@article{leebrooks2008,
author = {Lee, Benjamin C. and Brooks, David M.},
title = {Illustrative Design Space Studies with Microarchitectural Regression Models},
year = {2008},
issue_date = {February 2008},
publisher = {IEEE Computer Society},
address = {USA},
volume = {14},
number = {2},
issn = {1083-4427},
url = {https://doi.org/10.1109/HPCA.2008.4658640},
doi = {10.1109/HPCA.2008.4658640},
journal = {IEEE Micro},
month = mar,
pages = {76–88},
numpages = {13},
keywords = {design space exploration, microarchitecture, performance modeling, regression models}
}

