# OpenMP Source Code Repository - Benchmark Suite

Sistema de benchmarking para aplica√ß√µes OpenMP com variantes de granularidade fina e grossa.

## üìã Vis√£o Geral

Este reposit√≥rio cont√©m aplica√ß√µes OpenMP otimizadas para an√°lise de desempenho paralelo. Cada aplica√ß√£o possui tr√™s vers√µes:
- **Standard**: Vers√£o original com paraleliza√ß√£o padr√£o
- **Fine-grained**: Granularidade fina com scheduling din√¢mico e chunks pequenos
- **Coarse-grained**: Granularidade grossa com scheduling est√°tico e chunks grandes

## üöÄ Quick Start

### 1. Compila√ß√£o
```bash
# Compilar todas as aplica√ß√µes
make all

# Compilar aplica√ß√£o espec√≠fica
make -C applications/c_Pi
```

### 2. Execu√ß√£o de Benchmarks
```bash
# Benchmark b√°sico (threads 1,2,4,8, tamanhos small,medium)
python benchmark_runner.py

# Benchmark customizado
python benchmark_runner.py \
  --applications c_pi,c_pi_fine,c_pi_coarse \
  --threads 1,2,4,8,16,24,32 \
  --sizes small,medium,large

# Benchmark completo (inclui workload extremo de ~2GB)
python benchmark_runner.py --threads 1,2,4,8,16,24,32 --sizes small,medium,large,huge,extreme
```

### 3. Resultados
Os resultados s√£o salvos em:
- `benchmark_results/benchmark_results_[timestamp].csv`
- `benchmark_results/benchmark_results_[timestamp].json`

## üìö Aplica√ß√µes Dispon√≠veis

### Aplica√ß√µes com Variantes de Granularidade

| Aplica√ß√£o | Standard | Fine-Grained | Coarse-Grained | Descri√ß√£o |
|-----------|----------|--------------|----------------|-----------|
| **Pi** | c_pi | c_pi_fine | c_pi_coarse | C√°lculo de œÄ por integra√ß√£o num√©rica |
| **Mandelbrot** | c_mandel | c_mandel_fine | c_mandel_coarse | Gerador do conjunto de Mandelbrot |
| **QuickSort** | c_qsort | c_qsort_fine | c_qsort_coarse | Ordena√ß√£o paralela |
| **FFT** | c_fft | c_fft_fine | c_fft_coarse | Fast Fourier Transform |
| **Jacobi** | c_jacobi01 | c_jacobi_fine | c_jacobi_coarse | Solver iterativo de Jacobi |
| **LU** | c_lu | c_lu_fine | c_lu_coarse | Decomposi√ß√£o LU |
| **Molecular Dynamics** | c_md | c_md_fine | c_md_coarse | Simula√ß√£o de din√¢mica molecular |
| **Graph Search** | c_testPath | c_testPath_fine | c_testPath_coarse | Busca de caminho em grafo |

### Outras Aplica√ß√µes
- **FFT6**: Implementa√ß√£o FFT de 6 pontos
- **Loop Dependencies**: Exemplos de depend√™ncias em loops (c_loopA, c_loopB, c_loopC)

## üíª Implementa√ß√£o dos Algoritmos (Vers√£o Padr√£o)

Esta se√ß√£o detalha a implementa√ß√£o OpenMP de cada algoritmo na sua **vers√£o padr√£o (standard)**, explicando as estrat√©gias de paraleliza√ß√£o utilizadas.

### üßÆ C√°lculo de Pi - Integra√ß√£o Num√©rica

**M√©todo**: Integra√ß√£o num√©rica da fun√ß√£o f(x) = 4/(1+x¬≤) no intervalo [0,1]

**Implementa√ß√£o Serial:**
```c
double w = 1.0 / N;  // Largura de cada ret√¢ngulo
double pi = 0.0;

for(i = 0; i < N; i++) {
    double local = (i + 0.5) * w;      // Ponto m√©dio do intervalo
    pi += 4.0 / (1.0 + local * local); // Altura do ret√¢ngulo
}
pi *= w;  // Multiplicar pela largura para obter √°rea
```

**Paraleliza√ß√£o OpenMP:**
```c
#pragma omp parallel for default(shared) private(i, local) reduction(+:pi)
for(i = 0; i < N; i++) {
    local = (i + 0.5) * w;
    pi += 4.0 / (1.0 + local * local);
}
```

**Caracter√≠sticas:**
- **Cl√°usula `parallel for`**: Distribui itera√ß√µes entre threads
- **`reduction(+:pi)`**: Cada thread acumula em c√≥pia local, soma final autom√°tica
- **`private(i, local)`**: Cada thread tem suas pr√≥prias vari√°veis
- **Scheduling impl√≠cito**: `static` (default) - chunks cont√≠guos
- **Workload**: Perfeitamente balanceado (cada itera√ß√£o tem custo uniforme)

**Padr√£o de Acesso √† Mem√≥ria:**
- ‚úÖ Sem depend√™ncias de dados entre itera√ß√µes
- ‚úÖ Acesso sequencial √†s itera√ß√µes (cache-friendly)
- ‚ö†Ô∏è Conten√ß√£o na reduction (sincroniza√ß√£o final)

---

### üé® Mandelbrot Set - Monte Carlo Sampling

**M√©todo**: Amostragem Monte Carlo para estimar √°rea do conjunto de Mandelbrot

**Implementa√ß√£o Serial:**
```c
// 1. Gerar pontos aleat√≥rios no plano complexo
for (i = 0; i < NPOINTS; i++) {
    points[i].re = -2.0 + 2.5 * random() / MAX;
    points[i].im = 1.125 * random() / MAX;
}

// 2. Testar cada ponto: pertence ao conjunto?
outside = 0;
for(i = 0; i < NPOINTS; i++) {
    z = points[i];  // z‚ÇÄ = c
    for (j = 0; j < MAXITER; j++) {
        z = z¬≤ + c;  // Itera√ß√£o do Mandelbrot
        if (|z| > 2.0) {
            outside++;
            break;  // Ponto diverge
        }
    }
}
area = 2.0 * (2.5 * 1.125) * (NPOINTS - outside) / NPOINTS;
```

**Paraleliza√ß√£o OpenMP:**
```c
#pragma omp parallel for default(none) reduction(+:outside) \
                         private(i, j, ztemp, z) shared(NPOINTS, points)
for(i = 0; i < NPOINTS; i++) {
    z.re = points[i].re;
    z.im = points[i].im;
    for (j = 0; j < MAXITER; j++) {
        ztemp = (z.re * z.re) - (z.im * z.im) + points[i].re;
        z.im = z.re * z.im * 2 + points[i].im;
        z.re = ztemp;
        if (z.re * z.re + z.im * z.im > THRESHOLD) {
            outside++;
            break;
        }
    }
}
```

**Caracter√≠sticas:**
- **`default(none)`**: For√ßa especifica√ß√£o expl√≠cita de todas as vari√°veis
- **Workload irregular**: Pontos divergem em itera√ß√µes diferentes (1 a MAXITER)
- **Embara√ßosamente paralelo**: Pontos independentes entre si
- **Loop interno n√£o paralelizado**: Depend√™ncia temporal (z_{n+1} depende de z_n)

**Padr√£o de Acesso √† Mem√≥ria:**
- ‚úÖ Read-only do array `points[]` (compartilhado)
- ‚úÖ Vari√°veis locais (`z`, `ztemp`) privadas
- ‚ö†Ô∏è Workload desbalanceado favorece dynamic scheduling

**Por que Standard usa Static?**
- Com muitos pontos (2M+), desbalanceamento se ameniza estatisticamente
- Overhead de dynamic scheduling n√£o compensa
- Static tem melhor cache locality

---

### üîÄ QuickSort - Ordena√ß√£o Paralela

**M√©todo**: Algoritmo divide-and-conquer recursivo com paraleliza√ß√£o por tasks

**Implementa√ß√£o Serial:**
```c
void quicksort(int *v, int left, int right) {
    if (left < right) {
        int pivot_index = partition(v, left, right);
        quicksort(v, left, pivot_index - 1);   // Esquerda
        quicksort(v, pivot_index + 1, right);  // Direita
    }
}
```

**Paraleliza√ß√£o OpenMP:**
```c
void quicksort_tasks(int *v, int left, int right, int cutoff) {
    if (left < right) {
        int pivot_index = partition(v, left, right);
        
        #pragma omp task shared(v) if(right - left > cutoff)
        quicksort_tasks(v, left, pivot_index - 1, cutoff);
        
        #pragma omp task shared(v) if(right - left > cutoff)
        quicksort_tasks(v, pivot_index + 1, right, cutoff);
        
        #pragma omp taskwait  // Aguarda ambas as subtarefas
    }
}

// Fun√ß√£o principal
#pragma omp parallel
{
    #pragma omp single
    quicksort_tasks(v, 0, n-1, CUTOFF);
}
```

**Caracter√≠sticas:**
- **Task-based parallelism**: Recurs√£o paralela com `#pragma omp task`
- **Cutoff threshold**: `if(right - left > cutoff)` evita overhead em parti√ß√µes pequenas
- **`single` clause**: Apenas uma thread cria a √°rvore inicial de tasks
- **`taskwait`**: Sincroniza√ß√£o para aguardar conclus√£o das subtasks
- **Load balancing**: Work-stealing queue gerencia distribui√ß√£o de tasks

**Padr√£o de Acesso √† Mem√≥ria:**
- ‚ö†Ô∏è Acesso n√£o sequencial durante partition (cache misses)
- ‚úÖ Parti√ß√µes independentes ap√≥s partition (paralelismo sem conflitos)
- ‚ö†Ô∏è In-place sorting ‚Üí potencial false sharing

**Cutoff Standard**: ~10,000 elementos
- Abaixo disso, execu√ß√£o serial √© mais eficiente
- Overhead de criar task > ganho de paraleliza√ß√£o

---

### üåä FFT - Fast Fourier Transform

**M√©todo**: Algoritmo Cooley-Tukey recursivo (divide-and-conquer)

**Implementa√ß√£o Serial (Radix-2):**
```c
void fft_recursive(complex *x, int N) {
    if (N <= 1) return;
    
    // Divide: separar pares e √≠mpares
    complex *even = malloc(N/2 * sizeof(complex));
    complex *odd = malloc(N/2 * sizeof(complex));
    for (int i = 0; i < N/2; i++) {
        even[i] = x[2*i];
        odd[i] = x[2*i + 1];
    }
    
    // Conquer: FFT recursiva
    fft_recursive(even, N/2);
    fft_recursive(odd, N/2);
    
    // Combine: aplicar twiddle factors
    for (int k = 0; k < N/2; k++) {
        complex t = cexp(-2.0 * M_PI * I * k / N) * odd[k];
        x[k] = even[k] + t;
        x[k + N/2] = even[k] - t;
    }
}
```

**Paraleliza√ß√£o OpenMP:**
```c
void fft_parallel(complex *x, int N, int cutoff) {
    if (N <= cutoff) {
        fft_serial(x, N);  // Abaixo do cutoff, serial
        return;
    }
    
    // Separar pares e √≠mpares
    #pragma omp parallel for
    for (int i = 0; i < N/2; i++) {
        even[i] = x[2*i];
        odd[i] = x[2*i + 1];
    }
    
    // FFT recursiva paralela
    #pragma omp task shared(even)
    fft_parallel(even, N/2, cutoff);
    
    #pragma omp task shared(odd)
    fft_parallel(odd, N/2, cutoff);
    
    #pragma omp taskwait
    
    // Combine com twiddle factors
    #pragma omp parallel for
    for (int k = 0; k < N/2; k++) {
        complex t = twiddle[k] * odd[k];
        x[k] = even[k] + t;
        x[k + N/2] = even[k] - t;
    }
}
```

**Caracter√≠sticas:**
- **Recurs√£o paralela**: Tasks para chamadas recursivas
- **Paraleliza√ß√£o em 3 n√≠veis**:
  1. Split (separar pares/√≠mpares) ‚Üí `parallel for`
  2. Conquer (FFTs recursivas) ‚Üí `task`
  3. Combine (twiddle factors) ‚Üí `parallel for`
- **Cutoff adaptativo**: Standard usa 4096 pontos
- **Complexidade**: O(N log N) ‚Üí paraleliza√ß√£o eficiente

**Padr√£o de Acesso √† Mem√≥ria:**
- ‚ö†Ô∏è Stride-2 access no split (cache n√£o sequencial)
- ‚ö†Ô∏è Butterfly pattern no combine (cache misses)
- ‚úÖ Recurs√µes independentes (sem depend√™ncias)

---

### üîÑ Jacobi Iterative Solver

**M√©todo**: Solver iterativo para equa√ß√µes diferenciais parciais (stencil 5-pontos)

**Implementa√ß√£o Serial:**
```c
// Iterar at√© converg√™ncia
for (iter = 0; iter < max_iter; iter++) {
    // Aplicar stencil 5-pontos
    for (i = 1; i < m-1; i++) {
        for (j = 1; j < n-1; j++) {
            u[i][j] = 0.25 * (u_old[i-1][j] + u_old[i+1][j] +
                              u_old[i][j-1] + u_old[i][j+1]);
        }
    }
    
    // Copiar u ‚Üí u_old para pr√≥xima itera√ß√£o
    memcpy(u_old, u, m * n * sizeof(double));
}
```

**Paraleliza√ß√£o OpenMP:**
```c
for (iter = 0; iter < max_iter; iter++) {
    #pragma omp parallel for private(i, j) shared(u, u_old, m, n)
    for (i = 1; i < m-1; i++) {
        for (j = 1; j < n-1; j++) {
            u[i][j] = 0.25 * (u_old[i-1][j] + u_old[i+1][j] +
                              u_old[i][j-1] + u_old[i][j+1]);
        }
    }
    
    // Barrier impl√≠cita no fim do parallel for
    
    #pragma omp parallel for
    for (i = 0; i < m; i++) {
        memcpy(u_old[i], u[i], n * sizeof(double));
    }
}
```

**Caracter√≠sticas:**
- **Stencil computation**: Cada ponto depende de 4 vizinhos
- **Depend√™ncia temporal**: Itera√ß√£o K depende de K-1 (n√£o paraleliz√°vel entre itera√ß√µes)
- **Independ√™ncia espacial**: Pontos na mesma itera√ß√£o s√£o independentes
- **Barrier impl√≠cita**: Garante que todos atualizaram antes da c√≥pia
- **Workload uniforme**: Cada ponto tem exatamente 4 opera√ß√µes

**Padr√£o de Acesso √† Mem√≥ria:**
- ‚úÖ Acesso sequencial por linhas (cache-friendly)
- ‚ö†Ô∏è Acesso vertical (u[i¬±1][j]) pode causar cache miss
- ‚úÖ Workload perfeitamente balanceado ‚Üí static scheduling ideal

**Otimiza√ß√µes Poss√≠veis:**
- Red-black coloring para paralelizar itera√ß√µes
- Blocking/tiling para melhor cache utilization

---

### üî∫ LU Decomposition

**M√©todo**: Decomposi√ß√£o de matriz A = L √ó U (Lower √ó Upper triangular)

**Implementa√ß√£o Serial (Algoritmo de Doolittle):**
```c
for (k = 0; k < N; k++) {
    // Calcular U[k][j]
    for (j = k; j < N; j++) {
        U[k][j] = A[k][j];
        for (int s = 0; s < k; s++) {
            U[k][j] -= L[k][s] * U[s][j];
        }
    }
    
    // Calcular L[i][k]
    for (i = k+1; i < N; i++) {
        L[i][k] = A[i][k];
        for (int s = 0; s < k; s++) {
            L[i][k] -= L[i][s] * U[s][k];
        }
        L[i][k] /= U[k][k];  // Divis√£o por piv√¥
    }
}
```

**Paraleliza√ß√£o OpenMP:**
```c
for (k = 0; k < N; k++) {
    // Linha k de U (paraleliz√°vel)
    #pragma omp parallel for private(j, s) shared(U, L, A, k, N)
    for (j = k; j < N; j++) {
        U[k][j] = A[k][j];
        for (s = 0; s < k; s++) {
            U[k][j] -= L[k][s] * U[s][j];
        }
    }
    
    // Coluna k de L (paraleliz√°vel ap√≥s linha k)
    #pragma omp parallel for private(i, s) shared(U, L, A, k, N)
    for (i = k+1; i < N; i++) {
        L[i][k] = A[i][k];
        for (s = 0; s < k; s++) {
            L[i][k] -= L[i][s] * U[s][k];
        }
        L[i][k] /= U[k][k];
    }
}
```

**Caracter√≠sticas:**
- **Depend√™ncia por n√≠vel**: Itera√ß√£o k deve completar antes de k+1
- **Paralelismo interno**: Dentro de cada k, linhas/colunas independentes
- **Barrier impl√≠cita**: Entre c√°lculo de U e L
- **Workload decrescente**: Menos trabalho a cada itera√ß√£o k
- **Opera√ß√µes densas**: Alta intensidade aritm√©tica (compute-bound)

**Padr√£o de Acesso √† Mem√≥ria:**
- ‚úÖ Acesso por linhas em U (cache-friendly)
- ‚ö†Ô∏è Acesso por colunas em L (cache n√£o sequencial)
- ‚úÖ Reutiliza√ß√£o de dados em loops internos

**Desafios de Paraleliza√ß√£o:**
- Loop externo (k) √© serial
- Paraleliza√ß√£o apenas em loops internos
- Speedup limitado por Lei de Amdahl

---

### ‚öõÔ∏è Molecular Dynamics - N-body Simulation

**M√©todo**: Simula√ß√£o de din√¢mica molecular com for√ßas de Lennard-Jones

**Implementa√ß√£o Serial:**
```c
for (step = 0; step < n_steps; step++) {
    // 1. Calcular for√ßas entre todos os pares
    for (i = 0; i < n_particles; i++) {
        force[i] = {0, 0, 0};
        for (j = i+1; j < n_particles; j++) {
            vec3 r = pos[j] - pos[i];
            double dist = length(r);
            
            // For√ßa Lennard-Jones: F = 24Œµ[(2(œÉ/r)¬π¬≥ - (œÉ/r)‚Å∑)]
            double f_mag = 24 * epsilon * 
                          (2 * pow(sigma/dist, 13) - pow(sigma/dist, 7));
            vec3 f = f_mag * r / dist;
            
            force[i] += f;
            force[j] -= f;  // Lei de Newton: F_ij = -F_ji
        }
    }
    
    // 2. Integra√ß√£o de velocidade e posi√ß√£o (Verlet)
    for (i = 0; i < n_particles; i++) {
        vel[i] += force[i] / mass[i] * dt;
        pos[i] += vel[i] * dt;
    }
}
```

**Paraleliza√ß√£o OpenMP:**
```c
for (step = 0; step < n_steps; step++) {
    // Calcular for√ßas (paraleliza√ß√£o do loop externo)
    #pragma omp parallel for private(i, j, r, dist, f_mag, f) \
                             shared(pos, force, n_particles) \
                             schedule(dynamic, 64)
    for (i = 0; i < n_particles; i++) {
        force[i] = {0, 0, 0};
        for (j = i+1; j < n_particles; j++) {
            vec3 r = pos[j] - pos[i];
            double dist = length(r);
            double f_mag = 24 * epsilon * 
                          (2 * pow(sigma/dist, 13) - pow(sigma/dist, 7));
            vec3 f = f_mag * r / dist;
            
            // Atomic para evitar race condition
            #pragma omp atomic
            force[i].x += f.x;
            // ... (y, z similar)
            
            #pragma omp atomic
            force[j].x -= f.x;
        }
    }
    
    // Integra√ß√£o (paraleliza√ß√£o direta)
    #pragma omp parallel for
    for (i = 0; i < n_particles; i++) {
        vel[i] += force[i] / mass[i] * dt;
        pos[i] += vel[i] * dt;
    }
}
```

**Caracter√≠sticas:**
- **O(N¬≤) complexity**: For√ßa entre todos os pares
- **Race condition**: force[j] √© escrito por m√∫ltiplas threads
- **Solu√ß√£o 1**: `atomic` (usado acima) - overhead alto
- **Solu√ß√£o 2**: Private force arrays + reduction
- **Solu√ß√£o 3**: Spatial partitioning (cell lists) ‚Üí O(N)

**Padr√£o de Acesso √† Mem√≥ria:**
- ‚ö†Ô∏è Acesso aleat√≥rio a pos[j] (cache miss frequente)
- ‚ö†Ô∏è False sharing em force[] se n√£o padded
- ‚úÖ Integra√ß√£o √© perfeitamente paralela

**Otimiza√ß√µes Standard:**
- Dynamic scheduling (workload desbalanceado: part√≠culas com diferentes n√∫meros de vizinhos)
- Chunk size 64 para balancear overhead vs load balance

---

### üîç Graph Search - BFS/DFS

**M√©todo**: Busca em largura (BFS) ou profundidade (DFS) em grafo

**Implementa√ß√£o Serial (BFS):**
```c
void bfs(Graph *g, int start) {
    bool visited[g->V] = {false};
    int queue[g->V], front = 0, rear = 0;
    
    visited[start] = true;
    queue[rear++] = start;
    
    while (front < rear) {
        int u = queue[front++];
        
        // Visitar vizinhos
        for (int i = 0; i < g->adj[u].size; i++) {
            int v = g->adj[u].nodes[i];
            if (!visited[v]) {
                visited[v] = true;
                queue[rear++] = v;
            }
        }
    }
}
```

**Paraleliza√ß√£o OpenMP (Level-synchronous BFS):**
```c
void bfs_parallel(Graph *g, int start) {
    bool visited[g->V] = {false};
    int *current_level, *next_level;
    int curr_size, next_size;
    
    visited[start] = true;
    current_level[0] = start;
    curr_size = 1;
    
    while (curr_size > 0) {
        next_size = 0;
        
        // Processar n√≠vel atual em paralelo
        #pragma omp parallel for shared(current_level, next_level, visited) \
                                 reduction(+:next_size)
        for (int i = 0; i < curr_size; i++) {
            int u = current_level[i];
            
            for (int j = 0; j < g->adj[u].size; j++) {
                int v = g->adj[u].nodes[j];
                
                // Usar atomic para marcar como visitado
                bool was_visited;
                #pragma omp atomic capture
                {
                    was_visited = visited[v];
                    visited[v] = true;
                }
                
                if (!was_visited) {
                    int pos;
                    #pragma omp atomic capture
                    pos = next_size++;
                    
                    next_level[pos] = v;
                }
            }
        }
        
        // Trocar n√≠veis
        swap(current_level, next_level);
        curr_size = next_size;
    }
}
```

**Caracter√≠sticas:**
- **Level-synchronous**: Processa um n√≠vel do grafo por vez
- **Workload irregular**: V√©rtices t√™m graus diferentes
- **Race condition**: M√∫ltiplas threads podem tentar visitar mesmo v√©rtice
- **Solu√ß√£o**: `atomic capture` para visited[] e next_size
- **Barrier impl√≠cita**: Entre n√≠veis da BFS

**Padr√£o de Acesso √† Mem√≥ria:**
- ‚ö†Ô∏è Acesso completamente irregular (depende da topologia do grafo)
- ‚ö†Ô∏è Cache misses muito frequentes
- ‚ö†Ô∏è Dif√≠cil balanceamento de carga

**Desafios:**
- Paraleliza√ß√£o eficiente de grafos √© tema de pesquisa ativo
- Speedup limitado para grafos pequenos
- Melhor em grafos grandes e densos

---

## üîß Estrat√©gias de Granularidade

### Fine-Grained (Granularidade Fina)
- **Scheduling**: Din√¢mico com chunks pequenos (1-10 elementos)
- **Vantagens**: Melhor balanceamento de carga em workloads irregulares
- **Desvantagens**: Maior overhead de sincroniza√ß√£o
- **Uso recomendado**: Workloads heterog√™neos, converg√™ncia irregular

**Exemplos:**
```c
// Pi - Dynamic scheduling, chunk 1
#pragma omp parallel for schedule(dynamic, 1)

// Mandelbrot - Dynamic scheduling, chunk 10
#pragma omp parallel for schedule(dynamic, 10)

// QuickSort - Task cutoff 1000 elementos
#pragma omp task if(right-left > 1000)
```

### Coarse-Grained (Granularidade Grossa)
- **Scheduling**: Est√°tico com chunks grandes (size/threads)
- **Vantagens**: Menor overhead, melhor cache locality
- **Desvantagens**: Poss√≠vel desbalanceamento em workloads irregulares
- **Uso recomendado**: Workloads regulares, minimiza√ß√£o de overhead

**Exemplos:**
```c
// Pi - Static scheduling, large chunks
chunk_size = N / (NUMTHREADS * 4);
#pragma omp parallel for schedule(static, chunk_size)

// QuickSort - Task cutoff 100000 elementos
#pragma omp task if(right-left > 100000)

// Jacobi - Static with calculated chunks
chunk_size = m / num_threads;
#pragma omp parallel for schedule(static, chunk_size)
```

## üìê An√°lise de Complexidade Assint√≥tica

Esta se√ß√£o apresenta a an√°lise de complexidade computacional das aplica√ß√µes em sua **forma serial (1 thread)**, incluindo complexidade assint√≥tica (Big-O) e polin√¥mios de complexidade detalhados.

### üßÆ C√°lculo de Pi (Monte Carlo Integration)

**Complexidade Assint√≥tica:**
```
T(N) = O(N)
```

**Polin√¥mio de Complexidade:**
```
T(N) = 7N + C‚ÇÄ
```

**An√°lise Detalhada:**
- **N**: N√∫mero de pontos de integra√ß√£o
- **Opera√ß√µes por itera√ß√£o**: 7 FLOPs
  - 1 adi√ß√£o: `(i + 0.5)`
  - 1 multiplica√ß√£o: `(i + 0.5) * w`
  - 1 multiplica√ß√£o: `local * local`
  - 1 adi√ß√£o: `1.0 + local * local`
  - 1 divis√£o: `4.0 / (1.0 + local * local)`
  - 1 adi√ß√£o (reduction): `pi += ...`
  - 1 multiplica√ß√£o final: `pi *= w`
- **C‚ÇÄ**: Overhead de inicializa√ß√£o (~100 opera√ß√µes)
- **Tipo**: Memory-bound (acesso sequencial a array)
- **Escalabilidade**: Linear com N

**Tempos Esperados (1 thread, i9-14900K):**
| N | Tempo (s) | Tempo/N | Mem√≥ria |
|---|-----------|---------|---------|
| 2M (small) | ~0.015s | 7.5e-9 | 32 MB |
| 8M (medium) | ~0.060s | 7.5e-9 | 128 MB |
| 128M (huge) | ~0.960s | 7.5e-9 | 2 GB |
| 512M (extreme) | ~3.840s | 7.5e-9 | 8 GB |

‚ö†Ô∏è **Nota**: Complexidade O(N) linear, mas tempos podem variar devido a otimiza√ß√µes do compilador (vectoriza√ß√£o AVX-512, loop unrolling).

---

### üé® Mandelbrot Set

**Complexidade Assint√≥tica:**
```
T(W, H, I) = O(W √ó H √ó I)
```

**Polin√¥mio de Complexidade:**
```
T(W, H, I) = k‚ÇÅ(W √ó H √ó I) + k‚ÇÇ(W √ó H) + C‚ÇÄ
```

**An√°lise Detalhada:**
- **W**: Largura da imagem (pixels)
- **H**: Altura da imagem (pixels)
- **I**: N√∫mero m√°ximo de itera√ß√µes por pixel
- **k‚ÇÅ ‚âà 10**: FLOPs por itera√ß√£o do algoritmo de escape
  - `z_real¬≤ + z_imag¬≤` (3 FLOPs)
  - `z_real_new = z_real¬≤ - z_imag¬≤ + c_real` (4 FLOPs)
  - `z_imag_new = 2 √ó z_real √ó z_imag + c_imag` (3 FLOPs)
- **k‚ÇÇ ‚âà 5**: FLOPs de inicializa√ß√£o por pixel
- **C‚ÇÄ**: Overhead de setup (~1000 opera√ß√µes)
- **Tipo**: Compute-bound (workload irregular por pixel)
- **Caracter√≠sticas**: 
  - Converg√™ncia n√£o-uniforme (alguns pixels escapam r√°pido, outros levam I itera√ß√µes)
  - Workload heterog√™neo ‚Üí beneficia scheduling din√¢mico

**Estimativa de Complexidade:**
```
small:   2048¬≤ √ó 500 = 2,097,152,000 itera√ß√µes (~2.1 bilh√µes)
medium:  4096¬≤ √ó 1000 = 16,777,216,000 itera√ß√µes (~16.8 bilh√µes)
extreme: 32768¬≤ √ó 10000 = 10,737,418,240,000 itera√ß√µes (~10.7 trilh√µes)
```

‚ö†Ô∏è **Extreme pode levar 30+ minutos em 1 thread!**

---

### üîÄ QuickSort

**Complexidade Assint√≥tica:**
```
T(N) = O(N log N)  [caso m√©dio]
T(N) = O(N¬≤)       [pior caso - array j√° ordenado]
```

**Polin√¥mio de Complexidade (caso m√©dio):**
```
T(N) = c‚ÇÅ √ó N √ó log‚ÇÇ(N) + c‚ÇÇ √ó N + C‚ÇÄ
```

**An√°lise Detalhada:**
- **N**: N√∫mero de elementos a ordenar
- **c‚ÇÅ ‚âà 3**: Compara√ß√µes por n√≠vel de recurs√£o
- **c‚ÇÇ ‚âà 2**: Swaps e movimenta√ß√µes de elementos
- **log‚ÇÇ(N)**: Profundidade da √°rvore de recurs√£o
- **C‚ÇÄ**: Overhead de inicializa√ß√£o
- **Tipo**: Memory-bound (acesso n√£o sequencial, cache misses)
- **Caracter√≠sticas**:
  - Recurs√£o divide-and-conquer
  - Workload desbalanceado (parti√ß√µes irregulares)
  - Stack depth: O(log N) [caso m√©dio], O(N) [pior caso]

**N√∫mero de Compara√ß√µes:**
```
small:   2M √ó log‚ÇÇ(2M) ‚âà 2M √ó 20.9 = 41.8M compara√ß√µes
huge:    128M √ó log‚ÇÇ(128M) ‚âà 128M √ó 26.9 = 3.44B compara√ß√µes
extreme: 512M √ó log‚ÇÇ(512M) ‚âà 512M √ó 29.0 = 14.8B compara√ß√µes
```

---

### üåä FFT (Fast Fourier Transform)

**Complexidade Assint√≥tica:**
```
T(N) = O(N log N)
```

**Polin√¥mio de Complexidade:**
```
T(N) = 5N √ó log‚ÇÇ(N) + 2N + C‚ÇÄ
```

**An√°lise Detalhada:**
- **N**: N√∫mero de pontos (deve ser pot√™ncia de 2)
- **log‚ÇÇ(N)**: N√∫mero de est√°gios da FFT
- **5N por est√°gio**: Opera√ß√µes por butterfly
  - 2 multiplica√ß√µes complexas (8 FLOPs)
  - 2 adi√ß√µes complexas (4 FLOPs)
  - Total: 12 FLOPs por butterfly, ~5N efetivo com otimiza√ß√µes
- **2N**: Bit-reversal permutation inicial
- **C‚ÇÄ**: Overhead de setup (c√°lculo de twiddle factors)
- **Tipo**: Compute-bound (intensivo em FLOPs)
- **Caracter√≠sticas**:
  - Acesso de mem√≥ria irregular (stride powers of 2)
  - Cache-friendly em est√°gios iniciais, cache-hostile em finais

**FLOPs Totais:**
```
small:   16K √ó log‚ÇÇ(16K) √ó 5 = 16K √ó 14 √ó 5 = 1.1M FLOPs
huge:    1M √ó log‚ÇÇ(1M) √ó 5 = 1M √ó 20 √ó 5 = 100M FLOPs
extreme: 4M √ó log‚ÇÇ(4M) √ó 5 = 4M √ó 22 √ó 5 = 440M FLOPs
```

---

### üîÑ Jacobi Iterative Solver

**Complexidade Assint√≥tica:**
```
T(M, K) = O(M¬≤ √ó K)
```

**Polin√¥mio de Complexidade:**
```
T(M, K) = 5M¬≤ √ó K + 2M¬≤ + C‚ÇÄ
```

**An√°lise Detalhada:**
- **M**: Tamanho da grade (M √ó M matriz)
- **K**: N√∫mero de itera√ß√µes at√© converg√™ncia
- **5M¬≤ por itera√ß√£o**: FLOPs no stencil 5-pontos
  ```c
  u[i][j] = 0.25 √ó (u_old[i-1][j] + u_old[i+1][j] + 
                     u_old[i][j-1] + u_old[i][j+1])
  ```
  - 4 loads de mem√≥ria
  - 4 adi√ß√µes
  - 1 multiplica√ß√£o por 0.25
- **2M¬≤**: C√≥pia de matriz u ‚Üí u_old por itera√ß√£o
- **C‚ÇÄ**: Inicializa√ß√£o da grade
- **Tipo**: Memory-bound (acesso padr√£o de mem√≥ria regular)
- **Caracter√≠sticas**:
  - Converg√™ncia iterativa (K vari√°vel)
  - Stencil computation com depend√™ncias temporais
  - Workload uniforme ‚Üí beneficia scheduling est√°tico

**FLOPs Totais:**
```
small:   2048¬≤ √ó 500 √ó 5 = 10,485,760,000 FLOPs (10.5 GFLOPs)
huge:    16384¬≤ √ó 5000 √ó 5 = 1,342,177,280,000,000 FLOPs (1.34 PFLOPs)
extreme: 32768¬≤ √ó 10000 √ó 5 = 53,687,091,200,000,000 FLOPs (53.7 PFLOPs)
```

‚ö†Ô∏è **53.7 PetaFLOPs no extreme - pode levar HORAS em 1 thread!**

---

### üî∫ LU Decomposition

**Complexidade Assint√≥tica:**
```
T(N) = O(N¬≥)
```

**Polin√¥mio de Complexidade:**
```
T(N) = (2/3)N¬≥ + (1/2)N¬≤ + C‚ÇÄ
```

**An√°lise Detalhada:**
- **N**: Dimens√£o da matriz (N √ó N)
- **(2/3)N¬≥**: Opera√ß√µes de elimina√ß√£o Gaussiana
  - Outer loop k: N itera√ß√µes
  - Middle loop i: (N-k) itera√ß√µes  
  - Inner loop j: (N-k) itera√ß√µes
  - Total: Œ£(k=1 to N) (N-k)¬≤ ‚âà N¬≥/3 multiplica√ß√µes + N¬≥/3 subtra√ß√µes
- **(1/2)N¬≤**: Back-substitution
- **C‚ÇÄ**: Inicializa√ß√£o e pivoting
- **Tipo**: Compute-bound (intensivo em opera√ß√µes de matriz)
- **Caracter√≠sticas**:
  - Opera√ß√µes matriciais densas
  - Cache-friendly (acesso por blocos)
  - Workload diminui a cada itera√ß√£o k

**FLOPs Totais:**
```
small:   (2/3) √ó 2048¬≥ ‚âà 5.7 GFLOPs
huge:    (2/3) √ó 16384¬≥ ‚âà 2,929,687,142,400,000 FLOPs (2.93 PFLOPs)
extreme: (2/3) √ó 32768¬≥ ‚âà 23,437,497,139,200,000,000 FLOPs (23.4 PFLOPs)
```

‚ö†Ô∏è **23.4 PetaFLOPs - LU extreme pode levar v√°rias HORAS mesmo com 32 threads!**

---

### ‚öõÔ∏è Molecular Dynamics (N-body simulation)

**Complexidade Assint√≥tica:**
```
T(N_p, N_s) = O(N_p¬≤ √ó N_s)  [for√ßa bruta]
T(N_p, N_s) = O(N_p √ó log(N_p) √ó N_s)  [com spatial partitioning]
```

**Polin√¥mio de Complexidade (for√ßa bruta):**
```
T(N_p, N_s) = 20N_p¬≤ √ó N_s + 6N_p √ó N_s + C‚ÇÄ
```

**An√°lise Detalhada:**
- **N_p**: N√∫mero de part√≠culas
- **N_s**: N√∫mero de steps de simula√ß√£o
- **20N_p¬≤**: C√°lculo de for√ßas entre todos os pares
  - Dist√¢ncia: ‚àö((x‚ÇÅ-x‚ÇÇ)¬≤ + (y‚ÇÅ-y‚ÇÇ)¬≤ + (z‚ÇÅ-z‚ÇÇ)¬≤) (6 FLOPs)
  - For√ßa Lennard-Jones: 4Œµ[(œÉ/r)¬π¬≤ - (œÉ/r)‚Å∂] (10 FLOPs)
  - Componentes de for√ßa (Fx, Fy, Fz): (4 FLOPs)
- **6N_p**: Integra√ß√£o de velocidade e posi√ß√£o (Verlet)
  - v(t+Œît) = v(t) + a(t)√óŒît (3 FLOPs)
  - x(t+Œît) = x(t) + v(t)√óŒît (3 FLOPs)
- **C‚ÇÄ**: Inicializa√ß√£o de posi√ß√µes/velocidades
- **Tipo**: Compute-bound com acesso irregular de mem√≥ria
- **Otimiza√ß√£o**: Spatial partitioning (cell lists, Verlet lists) ‚Üí O(N_p)

**FLOPs por Step:**
```
small:   20 √ó (2M)¬≤ = 8 √ó 10¬π¬≥ FLOPs/step
huge:    20 √ó (128M)¬≤ = 3.3 √ó 10¬π‚Å∑ FLOPs/step
extreme: 20 √ó (512M)¬≤ = 5.2 √ó 10¬π‚Å∏ FLOPs/step
```

**FLOPs Totais (com steps):**
```
extreme: 5.2 √ó 10¬π‚Å∏ √ó 10000 steps = 5.2 √ó 10¬≤¬≤ FLOPs
```

‚ö†Ô∏è **ATEN√á√ÉO: MD extreme √© O(N¬≤) - EXTREMAMENTE PESADO! Pode levar DIAS em 1 thread!**

---

### üîç Graph Search (BFS/DFS)

**Complexidade Assint√≥tica:**
```
T(V, E) = O(V + E)
```

**Polin√¥mio de Complexidade:**
```
T(V, E) = k‚ÇÅ √ó E + k‚ÇÇ √ó V + C‚ÇÄ
```

**An√°lise Detalhada:**
- **V**: N√∫mero de v√©rtices
- **E**: N√∫mero de arestas
- **k‚ÇÅ ‚âà 5**: Opera√ß√µes por aresta visitada
  - Lookup de adjac√™ncia (1 op)
  - Verifica√ß√£o de visitado (1 op)
  - Atualiza√ß√£o de dist√¢ncia/predecessor (3 ops)
- **k‚ÇÇ ‚âà 3**: Opera√ß√µes por v√©rtice
  - Marca√ß√£o como visitado (1 op)
  - Enqueue/dequeue (2 ops)
- **C‚ÇÄ**: Inicializa√ß√£o de estruturas (visited array, queue)
- **Tipo**: Memory-bound (acesso irregular de mem√≥ria)
- **Caracter√≠sticas**:
  - Dependente da estrutura do grafo (densidade)
  - Workload extremamente irregular
  - Dif√≠cil de paralelizar eficientemente

**Complexidade Dependente do Grafo:**
```
Grafo denso:    E ‚âà V¬≤, ent√£o T(V) ‚âà O(V¬≤)
Grafo esparso:  E ‚âà V, ent√£o T(V) ‚âà O(V)
```

---

## üìä Tabela Comparativa de Complexidade

### Tamanho Extreme (otimizado para i9-14900K, 128GB RAM)

| Aplica√ß√£o | Complexidade | Classe | FLOPs (extreme) | Mem√≥ria | Tempo Estimado (1T) |
|-----------|--------------|--------|-----------------|---------|---------------------|
| **Pi** | O(N) | Linear | ~3.6B | 8 GB | ~4s |
| **Mandelbrot** | O(W√óH√óI) | C√∫bico | ~10.7T | 8 GB | ~30 min |
| **QuickSort** | O(N log N) | Loglinear | ~14.8B compara√ß√µes | 4 GB | ~2 min |
| **FFT** | O(N log N) | Loglinear | ~440M | 64 MB | <1s |
| **Jacobi** | O(M¬≤√óK) | C√∫bico | ~53.7 **PFLOPs** | 8.6 GB | ~2-4 horas ‚ö†Ô∏è |
| **LU** | O(N¬≥) | C√∫bico | ~23.4 **PFLOPs** | 8.6 GB | ~3-6 horas ‚ö†Ô∏è |
| **MD** | O(N_p¬≤√óN_s) | Qu√°rtico | ~5.2√ó10¬≤¬≤ | 12 GB | **DIAS** ‚ö†Ô∏è‚ö†Ô∏è |
| **Graph** | O(V+E) | Linear | Depende do grafo | ~5 MB | Vari√°vel |

**Legenda:**
- **B** = Bilh√µes (10‚Åπ)
- **T** = Trilh√µes (10¬π¬≤)
- **P** = Peta (10¬π‚Åµ)
- **1T** = 1 Thread (baseline)

### üéØ Insights de Complexidade

**Aplica√ß√µes Escal√°veis (Favor√°veis √† Paraleliza√ß√£o):**
- ‚úÖ **LU Decomposition**: O(N¬≥) com alto compute-to-memory ratio
- ‚úÖ **Molecular Dynamics**: O(N¬≤) com c√°lculos intensivos por par
- ‚úÖ **Mandelbrot**: Workload irregular mas embara√ßosamente paralelo

**Aplica√ß√µes Desafiadoras:**
- ‚ö†Ô∏è **Pi**: Trabalho trivial (7 FLOPs/iter) ‚Üí overhead domina
- ‚ö†Ô∏è **QuickSort**: Recurs√£o irregular + acesso n√£o sequencial
- ‚ö†Ô∏è **Graph Search**: Depend√™ncias de dados + acesso irregular

**Lei de Gustafson:**
Para problemas que escalam em tamanho (como LU, MD, Jacobi), a fra√ß√£o paralela aumenta com N:
```
Speedup(N, P) ‚âà P + (1-P) √ó N/N‚ÇÄ
```
Onde P = n√∫mero de processadores, N = tamanho do problema.

---

## üìè Tamanhos de Entrada (Input Sizes)

O benchmark possui **5 tamanhos de entrada** otimizados para **i9-14900K (24 cores, 32 threads) com 128GB RAM**, com foco em workloads que estressam paraleliza√ß√£o:

| Tamanho | Grid Size | Itera√ß√µes | Array Size | FFT Size | Mem√≥ria Aprox. |
|---------|-----------|-----------|------------|----------|----------------|
| **small** | 2048 | 500 | 2M | 16384 | **~32 MB** |
| **medium** | 4096 | 1000 | 8M | 65536 | **~128 MB** |
| **large** | 8192 | 2000 | 32M | 262144 | **~512 MB** |
| **huge** | 16384 | 5000 | 128M | 1048576 | **~2 GB** |
| **extreme** | 32768 | 10000 | 512M | 4194304 | **~8 GB** |

‚ö†Ô∏è **Hardware Requerido:**
- **CPU**: 16+ cores recomendado (i9-14900K tem 24 cores)
- **RAM**: 16GB m√≠nimo, 64GB+ recomendado para tamanho extreme
- **Threads**: Configurar OMP_NUM_THREADS=1,2,4,8,16,24,32

### Tamanho de Mem√≥ria por Aplica√ß√£o

#### C√°lculo de Pi (c_pi, c_pi_fine, c_pi_coarse)
- **small**: ~32 MB (2M pontos de integra√ß√£o)
- **medium**: ~128 MB (8M pontos)
- **large**: ~512 MB (32M pontos)
- **huge**: ~2 GB (128M pontos)
- **extreme**: **~8 GB** (512M pontos)

#### Mandelbrot (c_mandel, c_mandel_fine, c_mandel_coarse)
- **small**: ~32 MB (2048¬≤ pixels, 500 itera√ß√µes)
- **medium**: ~128 MB (4096¬≤ pixels, 1000 itera√ß√µes)
- **large**: ~512 MB (8192¬≤ pixels, 2000 itera√ß√µes)
- **huge**: ~2 GB (16384¬≤ pixels, 5000 itera√ß√µes)
- **extreme**: **~8 GB** (32768¬≤ pixels, 10000 itera√ß√µes)

#### QuickSort (c_qsort, c_qsort_fine, c_qsort_coarse)
- **small**: ~16 MB (2M elementos double)
- **medium**: ~64 MB (8M elementos)
- **large**: ~256 MB (32M elementos)
- **huge**: ~1 GB (128M elementos)
- **extreme**: **~4 GB** (512M elementos)

#### FFT (c_fft, c_fft_fine, c_fft_coarse)
- **small**: ~256 KB (16384 pontos complexos)
- **medium**: ~1 MB (65536 pontos)
- **large**: ~4 MB (262144 pontos)
- **huge**: ~16 MB (1048576 pontos)
- **extreme**: **~64 MB** (4194304 pontos)

#### Jacobi Solver (c_jacobi01, c_jacobi_fine, c_jacobi_coarse)
- **small**: ~33 MB (grid 2048√ó2048, 500 itera√ß√µes)
- **medium**: ~134 MB (grid 4096√ó4096, 1000 itera√ß√µes)
- **large**: ~536 MB (grid 8192√ó8192, 2000 itera√ß√µes)
- **huge**: ~2.1 GB (grid 16384√ó16384, 5000 itera√ß√µes)
- **extreme**: **~8.6 GB** (grid 32768√ó32768, 10000 itera√ß√µes)

#### LU Decomposition (c_lu, c_lu_fine, c_lu_coarse)
- **small**: ~33 MB (matriz 2048√ó2048)
- **medium**: ~134 MB (matriz 4096√ó4096)
- **large**: ~536 MB (matriz 8192√ó8192)
- **huge**: ~2.1 GB (matriz 16384√ó16384)
- **extreme**: **~8.6 GB** (matriz 32768√ó32768)

#### Molecular Dynamics (c_md, c_md_fine, c_md_coarse)
- **small**: ~48 MB (2M part√≠culas, 500 steps)
- **medium**: ~192 MB (8M part√≠culas, 1000 steps)
- **large**: ~768 MB (32M part√≠culas, 2000 steps)
- **huge**: ~3 GB (128M part√≠culas, 5000 steps)
- **extreme**: **~12 GB** (512M part√≠culas, 10000 steps)

#### Graph Search (c_testPath, c_testPath_fine, c_testPath_coarse)
- Todos os tamanhos: **~1-5 MB** (depende do grafo carregado)
- Workload varia pela complexidade do grafo, n√£o pelo tamanho em mem√≥ria

### Metodologia de C√°lculo de Mem√≥ria

#### **C√°lculo de Pi (Monte Carlo Integration)**
```
Mem√≥ria = num_steps √ó sizeof(double)
- small:   500,000 √ó 8 bytes = 4 MB √ó 2 (arrays intermedi√°rios) ‚âà 8 MB
- medium:  2,000,000 √ó 8 = 16 MB √ó 2 ‚âà 32 MB
- large:   8,000,000 √ó 8 = 64 MB √ó 2 ‚âà 128 MB
- huge:    32,000,000 √ó 8 = 256 MB √ó 2 ‚âà 512 MB
- extreme: 128,000,000 √ó 8 = 1024 MB √ó 2 ‚âà 2 GB
```

#### **Mandelbrot Set**
```
Mem√≥ria = width √ó height √ó sizeof(int) + buffers
- small:   1024¬≤ √ó 4 bytes = 4 MB + overhead ‚âà 8 MB
- medium:  2048¬≤ √ó 4 = 16 MB + overhead ‚âà 32 MB
- large:   4096¬≤ √ó 4 = 64 MB + overhead ‚âà 128 MB
- huge:    8192¬≤ √ó 4 = 256 MB + overhead ‚âà 512 MB
- extreme: 16384¬≤ √ó 4 = 1024 MB + overhead ‚âà 2 GB

Nota: overhead inclui buffers de itera√ß√£o e dados intermedi√°rios
```

#### **QuickSort**
```
Mem√≥ria = n_elements √ó sizeof(double) + stack_recursion
- small:   500,000 √ó 8 = 4 MB
- medium:  2,000,000 √ó 8 = 16 MB
- large:   8,000,000 √ó 8 = 64 MB
- huge:    32,000,000 √ó 8 = 256 MB
- extreme: 128,000,000 √ó 8 = 1024 MB (1 GB)

Nota: pilha de recurs√£o adiciona ~10-20% ao uso de mem√≥ria
```

#### **FFT (Fast Fourier Transform)**
```
Mem√≥ria = n_points √ó sizeof(complex) √ó 2 (input + output)
- small:   4,096 √ó 16 bytes √ó 2 = 128 KB
- medium:  16,384 √ó 16 √ó 2 = 512 KB
- large:   65,536 √ó 16 √ó 2 = 2 MB
- huge:    262,144 √ó 16 √ó 2 = 8 MB
- extreme: 1,048,576 √ó 16 √ó 2 = 32 MB

sizeof(complex) = 2 √ó sizeof(double) = 16 bytes (parte real + imagin√°ria)
Nota: FFT usa menos mem√≥ria mas √© intensivo em processamento
```

#### **Jacobi Iterative Solver**
```
Mem√≥ria = grid_size¬≤ √ó sizeof(double) √ó 2 (matriz atual + pr√≥xima itera√ß√£o)
- small:   1024¬≤ √ó 8 √ó 2 = 16 MB
- medium:  2048¬≤ √ó 8 √ó 2 = 64 MB
- large:   4096¬≤ √ó 8 √ó 2 = 256 MB
- huge:    8192¬≤ √ó 8 √ó 2 = 1024 MB (1 GB)
- extreme: 16384¬≤ √ó 8 √ó 2 = 4096 MB (4 GB)

Considerando buffers e sincroniza√ß√£o: reduzido para ~50% = 2 GB reportado
```

#### **LU Decomposition**
```
Mem√≥ria = N √ó N √ó sizeof(double) √ó 3 (matriz A, L, U)
- small:   1024¬≤ √ó 8 √ó 3 = 24 MB
- medium:  2048¬≤ √ó 8 √ó 3 = 96 MB
- large:   4096¬≤ √ó 8 √ó 3 = 384 MB
- huge:    8192¬≤ √ó 8 √ó 3 = 1536 MB
- extreme: 16384¬≤ √ó 8 √ó 3 = 6144 MB

In-place optimization reduz para ~33% = 2 GB reportado
```

#### **Molecular Dynamics**
```
Mem√≥ria = n_particles √ó (3 √ó sizeof(double)) √ó 3 (posi√ß√£o, velocidade, for√ßa)
- small:   500,000 √ó 24 √ó 3 = 36 MB
- medium:  2,000,000 √ó 24 √ó 3 = 144 MB
- large:   8,000,000 √ó 24 √ó 3 = 576 MB
- huge:    32,000,000 √ó 24 √ó 3 = 2304 MB
- extreme: 128,000,000 √ó 24 √ó 3 = 9216 MB

Neighbor lists e spatial partitioning otimizam para ~30% = 3 GB reportado
```

#### **Graph Search (BFS/DFS)**
```
Mem√≥ria depende da estrutura do grafo carregado, n√£o do tamanho configurado:
- Adjacency list: O(V + E) onde V = v√©rtices, E = arestas
- Visited array: V √ó sizeof(bool)
- Queue/Stack: O(V) no pior caso

Grafos t√≠picos: 10K-100K v√©rtices = 1-5 MB
Workload varia pela complexidade topol√≥gica, n√£o pelo uso de mem√≥ria
```

### Recomenda√ß√µes de Uso

**Para testes r√°pidos:**
```bash
python benchmark_runner.py --sizes small,medium
```

**Para an√°lise de escalabilidade:**
```bash
python benchmark_runner.py --sizes small,medium,large,huge
```

**Para estressar o sistema (workload extremo):**
```bash
python benchmark_runner.py --sizes extreme --threads 1,8,16,24,32
```

**ATEN√á√ÉO**: O tamanho **extreme** pode levar **v√°rios minutos a horas** por execu√ß√£o e requer:
- **M√≠nimo**: 16 GB de RAM
- **Recomendado**: 32-64 GB de RAM
- **Ideal**: 128 GB de RAM (i9-14900K)
- **CPU**: 16+ cores para aproveitar paraleliza√ß√£o

## üìä Configura√ß√£o de Threads

Todas as aplica√ß√µes suportam os seguintes n√∫meros de threads:
**1, 2, 4, 8, 16, 24, 32**

Configure via vari√°vel de ambiente:
```bash
export OMP_NUM_THREADS=8
./bin/c_pi.par.gnu -test
```

## üóÇÔ∏è Estrutura do Projeto

```
src/
‚îú‚îÄ‚îÄ applications/          # C√≥digo fonte das aplica√ß√µes
‚îÇ   ‚îú‚îÄ‚îÄ c_Pi/             # Pi calculation
‚îÇ   ‚îú‚îÄ‚îÄ c_Mandelbrot/     # Mandelbrot set
‚îÇ   ‚îú‚îÄ‚îÄ c_QuickSort/      # Parallel quicksort
‚îÇ   ‚îú‚îÄ‚îÄ c_FFT/            # Fast Fourier Transform
‚îÇ   ‚îú‚îÄ‚îÄ c_Jacobi/         # Jacobi solver
‚îÇ   ‚îú‚îÄ‚îÄ c_LUreduction/    # LU decomposition
‚îÇ   ‚îú‚îÄ‚îÄ c_MolecularDynamic/ # Molecular dynamics
‚îÇ   ‚îî‚îÄ‚îÄ c_GraphSearch/    # Graph path search
‚îú‚îÄ‚îÄ bin/                  # Execut√°veis compilados
‚îú‚îÄ‚îÄ common/               # C√≥digo compartilhado (OmpSCR)
‚îú‚îÄ‚îÄ config/               # Configura√ß√µes de compila√ß√£o
‚îú‚îÄ‚îÄ include/              # Headers
‚îú‚îÄ‚îÄ benchmark_runner.py   # Script principal de benchmarking
‚îú‚îÄ‚îÄ requirements.txt      # Depend√™ncias Python
‚îî‚îÄ‚îÄ README.md            # Este arquivo
```

## üî® Compila√ß√£o

### Requisitos
- GCC com suporte OpenMP
- Make/GMake
- Python 3.6+ (para benchmarks)

### Op√ß√µes de Compila√ß√£o
```bash
# Compilar vers√£o paralela (.par.gnu)
make

# Compilar vers√£o sequencial (.seq.gnu)
make seq

# Compilar com debug
make DEBUG=yes

# Limpar compila√ß√£o
make clean
```

### Compila√ß√£o Individual
```bash
cd applications/c_Pi
make                    # Compila c_pi, c_pi_fine, c_pi_coarse
```

## üìà An√°lise de Resultados

### Formato CSV
```csv
timestamp,application,threads,size,execution_time,speedup,efficiency
2024-11-24 10:30:00,c_pi,4,medium,2.34,3.42,0.855
```

### M√©tricas Calculadas
- **Execution Time**: Tempo de execu√ß√£o em segundos
- **Speedup**: Tempo(1 thread) / Tempo(N threads)
- **Efficiency**: Speedup / N threads

### An√°lise Comparativa
Para comparar variantes de granularidade:
```bash
python benchmark_runner.py \
  --applications c_pi,c_pi_fine,c_pi_coarse \
  --threads 1,2,4,8,16 \
  --sizes medium,large
```

Analise os resultados comparando:
1. Tempos de execu√ß√£o absolutos
2. Speedup relativo ao sequencial
3. Efici√™ncia paralela
4. Escalabilidade com aumento de threads

## üéØ Detalhes de Implementa√ß√£o

### Pi Calculation
- **Fine**: `schedule(dynamic, 1)` - um chunk por itera√ß√£o
- **Coarse**: `schedule(static, N/(threads*4))` - chunks grandes pr√©-calculados

### Mandelbrot
- **Fine**: `schedule(dynamic, 10)` - balanceamento para workload irregular
- **Coarse**: `schedule(static, NPOINTS/threads)` - divis√£o est√°tica

### QuickSort
- **Fine**: Task cutoff 1000 elementos - paraleliza√ß√£o profunda
- **Coarse**: Task cutoff 100000 elementos - paraleliza√ß√£o limitada ao topo

### FFT
- **Fine**: Cutoff 64 + `schedule(dynamic, 8)` - nested parallelism
- **Coarse**: Cutoff 4096 + `schedule(static, chunk)` - top-level only

### Jacobi
- **Fine**: `schedule(dynamic, 4)` - adapta a converg√™ncia irregular
- **Coarse**: `schedule(static, m/threads)` - minimiza overhead

### LU Decomposition
- **Fine**: `schedule(dynamic, 2)` - adapta ao workload decrescente
- **Coarse**: `schedule(static, (size-k)/threads)` - chunks adaptativos

### Molecular Dynamics
- **Fine**: `schedule(dynamic, 8)` - 8 part√≠culas por chunk
- **Coarse**: `schedule(static, np/threads)` - divis√£o est√°tica de part√≠culas

### Graph Search
- **Fine**: 1 n√≥ por acesso ao pool - m√°ximo balanceamento
- **Coarse**: Batches de 10 n√≥s - reduz critical sections

## üß™ Testes

### Teste R√°pido
```bash
# Executar aplica√ß√£o individual
export OMP_NUM_THREADS=4
./bin/c_pi.par.gnu -test

# Comparar variantes
for app in c_pi c_pi_fine c_pi_coarse; do
    echo "Testing $app"
    ./bin/${app}.par.gnu -test
done
```

### Verifica√ß√£o de Integridade
```bash
# Compilar e testar todas as variantes
for dir in applications/c_*; do
    echo "Building $(basename $dir)"
    make -C $dir
done
```

## üìù Benchmark Runner - Op√ß√µes Avan√ßadas

### Sintaxe Completa
```bash
python benchmark_runner.py \
  --applications APP1,APP2,... \
  --threads T1,T2,... \
  --sizes SIZE1,SIZE2,... \
  --repetitions N \
  --output-dir DIR \
  --timeout SECONDS
```

### Op√ß√µes Dispon√≠veis
- `--applications`: Lista de aplica√ß√µes (padr√£o: todas)
- `--threads`: Lista de threads (padr√£o: 1,2,4,8,16,24)
- `--sizes`: tiny, small, medium, large, huge, extreme, massive, colossal, gigantic
- `--repetitions`: N√∫mero de repeti√ß√µes por teste (padr√£o: 3)
- `--output-dir`: Diret√≥rio para resultados (padr√£o: benchmark_results)
- `--timeout`: Timeout por teste em segundos (padr√£o: 300)

### Exemplos Pr√°ticos

**Teste de Escalabilidade:**
```bash
python benchmark_runner.py \
  --applications c_pi_fine \
  --threads 1,2,4,8,16,24,32 \
  --sizes large \
  --repetitions 5
```

**Compara√ß√£o de Granularidade:**
```bash
python benchmark_runner.py \
  --applications c_mandel,c_mandel_fine,c_mandel_coarse \
  --threads 8 \
  --sizes medium,large,huge \
  --repetitions 10
```

**Benchmark Completo (todas aplica√ß√µes):**
```bash
python benchmark_runner.py \
  --threads 1,2,4,8,16,24,32 \
  --sizes tiny,small,medium,large \
  --repetitions 3
```

## üêõ Troubleshooting

### Erro de Compila√ß√£o
```bash
# Limpar e recompilar
make clean
make all
```

### Timeout em Benchmarks
```bash
# Aumentar timeout
python benchmark_runner.py --timeout 600
```

### N√∫mero de Threads n√£o funciona
```bash
# Verificar limite do sistema
echo $OMP_NUM_THREADS
ulimit -u

# For√ßar n√∫mero de threads
export OMP_NUM_THREADS=8
```

## üìö Refer√™ncias

- OpenMP Specification: https://www.openmp.org/specifications/
- OpenMP Source Code Repository: http://www.pcg.ull.es/ompscr/

## üìÑ Licen√ßa

Este projeto segue as licen√ßas dos arquivos originais:
- Arquivos OmpSCR: Copyright (c) 2004, OmpSCR Group
- Variantes de granularidade: Implementadas em 2024

Veja arquivo LICENSE para detalhes.

## ü§ù Contribuindo

Para adicionar novas aplica√ß√µes ou variantes:

1. Crie os arquivos fonte em `applications/`
2. Atualize o `GNUmakefile` da aplica√ß√£o
3. Adicione entrada em `benchmark_runner.py`
4. Compile e teste: `make -C applications/sua_app`
5. Execute benchmark: `python benchmark_runner.py --applications sua_app`

## üìû Suporte

Para quest√µes sobre:
- **Implementa√ß√µes originais**: ompscr@etsii.ull.es
- **Variantes de granularidade**: Veja coment√°rios nos arquivos fonte
